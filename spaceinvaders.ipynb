{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import AtariDataset\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reseed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "seed = 42\n",
    "reseed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1960]\n"
     ]
    }
   ],
   "source": [
    "dataloader = AtariDataset(\"atari_v1\")\n",
    "observations, actions, rewards, next_observations, dones = dataloader.compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(210, 160)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_id, seed=25):\n",
    "    env = gym.make(env_id, obs_type='grayscale', render_mode=None, repeat_action_probability=0.15,frameskip=1)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env\n",
    "env = make_env(\"SpaceInvaders-v0\", seed=seed)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN DQN (TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m ACTION_SIZE \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m      7\u001b[0m dqn_learner \u001b[38;5;241m=\u001b[39m DQN(INPUT_SHAPE, ACTION_SIZE)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn_learner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_observations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dqn_test.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SPRING2024/CS4756/CS4756_FinalProj_SpaceInvader/dqn.py:133\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(network, env, observations, actions, rewards, next_observations, dones, save_path, batch_size, num_episodes, lr, add_data_every, device)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_episodes)):\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Add new data to the dataset after the first epoch and every 'add_data_every' epochs\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m add_data_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Collect new data\u001b[39;00m\n\u001b[1;32m    134\u001b[0m         data\u001b[38;5;241m.\u001b[39mextend(new_data)  \u001b[38;5;66;03m# Add new data to the dataset\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Shuffle the dataset before each epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/SPRING2024/CS4756/CS4756_FinalProj_SpaceInvader/dqn.py:188\u001b[0m, in \u001b[0;36mcollect_data\u001b[0;34m(network, env, num_episodes)\u001b[0m\n\u001b[1;32m    186\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 188\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Greedy action\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     next_obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    192\u001b[0m     new_data\u001b[38;5;241m.\u001b[39mappend((obs\u001b[38;5;241m.\u001b[39mflatten(), action, reward, next_obs\u001b[38;5;241m.\u001b[39mflatten(), done))\n",
      "File \u001b[0;32m~/SPRING2024/CS4756/CS4756_FinalProj_SpaceInvader/dqn.py:92\u001b[0m, in \u001b[0;36mDQN.get_action\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, q_values\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mq_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from dqn import DQN\n",
    "import dqn\n",
    "\n",
    "INPUT_SHAPE = 210*160\n",
    "ACTION_SIZE = env.action_space.n\n",
    "\n",
    "dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(dqn_learner, env, observations=observations, actions=actions, rewards=rewards, next_observations=next_observations, dones=dones, save_path='models/dqn_test.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the learner\n",
      "Training for 40 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:18<11:53, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2328628909548205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:36<11:29, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.09765370158450021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:53<10:57, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.08236520489168753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [01:10<10:21, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.07443511830046087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [01:26<09:54, 16.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.0692666074950839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [01:43<09:31, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.06509735768685687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [01:59<09:11, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.06211118761202208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [02:16<08:52, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.059370186522892825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [02:32<08:36, 16.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.056966968130095344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [02:49<08:22, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.05515189998744088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [03:07<08:13, 17.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.05301044978300527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [03:25<08:06, 17.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.05154143571335071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [03:42<07:43, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.050305204287308634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [04:00<07:36, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.048640288734539745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [04:17<07:12, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.047475668442041874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [04:34<06:50, 17.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.04637358335841032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [04:50<06:28, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.04509320502866254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [05:07<06:11, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.044029522883946966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [05:24<05:53, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.04287427127895116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [05:41<05:37, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.04217651392170204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [05:57<05:18, 16.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.04118504779411436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [06:14<05:01, 16.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 0.04024586619456568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [06:30<04:43, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 0.03981901526374851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [06:47<04:27, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 0.038817869599454845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [07:04<04:09, 16.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 0.0381938560814916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [07:20<03:52, 16.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 0.037224467491755835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [07:37<03:35, 16.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 0.03671300086403454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [07:54<03:23, 16.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 0.03613608223755015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [08:13<03:10, 17.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 0.03564826463161374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [08:32<02:57, 17.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 0.03460118074141664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [08:50<02:40, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.03443758685694685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [09:07<02:20, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 0.03386444329452856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [09:23<02:01, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 0.03322192889198798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [09:41<01:44, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 0.03271194785238533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [09:57<01:25, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: 0.03248046584419731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [10:14<01:07, 16.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: 0.03174151636866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [10:31<00:50, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 0.03136699610121907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [10:49<00:34, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 0.030783186944443715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [11:08<00:17, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 0.030496925144869083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [11:26<00:00, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: 0.029993618590682898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpaceInvLearner(\n",
       "  (fc1): Linear(in_features=33600, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc_out): Linear(in_features=256, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bc import SpaceInvLearner\n",
    "import bc\n",
    "\n",
    "learner = SpaceInvLearner(env)\n",
    "\n",
    "bc.train(learner=learner, observations=observations, checkpoint_path=\"models/bc_learner.pth\", actions=actions, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29296/2828031522.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  action = learner.get_action(torch.Tensor([obs]).to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186.0\n"
     ]
    }
   ],
   "source": [
    "learner.load_state_dict(torch.load(\"models/bc_learner.pth\"), strict=True)\n",
    "total_learner_reward = 0\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(20):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_learner_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(total_learner_reward/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD EXPERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expert.ppo import PPOAgent, ActorCnn, CriticCnn\n",
    "\n",
    "INPUT_SHAPE = (4, 84, 84)\n",
    "ACTION_SIZE = env.action_space.n\n",
    "SEED = 0\n",
    "GAMMA = 0.99           # discount factor\n",
    "ALPHA= 0.00001         # Actor learning rate\n",
    "BETA = 0.00001          # Critic learning rate\n",
    "TAU = 0.95\n",
    "BATCH_SIZE = 64\n",
    "PPO_EPOCH = 10\n",
    "CLIP_PARAM = 0.2\n",
    "UPDATE_EVERY = 1000    # how often to update the network \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = PPOAgent(INPUT_SHAPE, ACTION_SIZE, SEED, device, GAMMA, ALPHA, BETA, TAU, UPDATE_EVERY, BATCH_SIZE, PPO_EPOCH, CLIP_PARAM, ActorCnn(INPUT_SHAPE, ACTION_SIZE), CriticCnn(INPUT_SHAPE))\n",
    "agent.load_model(\"models/expert_actor.pth\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAgger Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After interaction 0, reward = 160.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.09196897419218451\n",
      "Epoch 1, Loss: 0.023473032517533327\n",
      "Epoch 2, Loss: 0.013229532919080743\n",
      "Epoch 3, Loss: 0.00898487860300569\n",
      "Epoch 4, Loss: 0.006821882086250559\n",
      "Epoch 5, Loss: 0.0057289537740347314\n",
      "Epoch 6, Loss: 0.00484704977706276\n",
      "Epoch 7, Loss: 0.00433736169568926\n",
      "Epoch 8, Loss: 0.00392468279840345\n",
      "Epoch 9, Loss: 0.0036411496291156448\n",
      "Epoch 10, Loss: 0.003362798730235628\n",
      "Epoch 11, Loss: 0.003148196141088112\n",
      "Epoch 12, Loss: 0.0029302238041656586\n",
      "Epoch 13, Loss: 0.0027873978977331253\n",
      "Epoch 14, Loss: 0.002602497859374499\n",
      "Epoch 15, Loss: 0.002486137566886792\n",
      "Epoch 16, Loss: 0.00235297215092209\n",
      "Epoch 17, Loss: 0.0022793096824341636\n",
      "Epoch 18, Loss: 0.0022279966772979004\n",
      "Epoch 19, Loss: 0.002104322397026435\n",
      "Epoch 20, Loss: 0.00195839288265466\n",
      "Epoch 21, Loss: 0.0019096977975036438\n",
      "Epoch 22, Loss: 0.0018380036446931705\n",
      "Epoch 23, Loss: 0.00181110089767755\n",
      "Epoch 24, Loss: 0.0017506486854312658\n",
      "Epoch 25, Loss: 0.001730013394245175\n",
      "Epoch 26, Loss: 0.0016149669118499452\n",
      "Epoch 27, Loss: 0.0015770874355779227\n",
      "Epoch 28, Loss: 0.001567459597773864\n",
      "Epoch 29, Loss: 0.0015430860957605882\n",
      "Epoch 30, Loss: 0.0014740288231748213\n",
      "Epoch 31, Loss: 0.001520862999712435\n",
      "Epoch 32, Loss: 0.0014945490694736163\n",
      "Epoch 33, Loss: 0.00141554570426558\n",
      "Epoch 34, Loss: 0.001407415280415518\n",
      "Epoch 35, Loss: 0.001342748016384846\n",
      "Epoch 36, Loss: 0.0013543443125690106\n",
      "Epoch 37, Loss: 0.001360572979302374\n",
      "Epoch 38, Loss: 0.0013048818144288448\n",
      "Epoch 39, Loss: 0.0013165485521007137\n",
      "After interaction 1, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.01858145986755564\n",
      "Epoch 1, Loss: 0.005801331979862985\n",
      "Epoch 2, Loss: 0.003934951267026271\n",
      "Epoch 3, Loss: 0.0031806253679297343\n",
      "Epoch 4, Loss: 0.0028530893127535948\n",
      "Epoch 5, Loss: 0.002547220440741956\n",
      "Epoch 6, Loss: 0.0023682289365578703\n",
      "Epoch 7, Loss: 0.002228815911531689\n",
      "Epoch 8, Loss: 0.0019829759511367636\n",
      "Epoch 9, Loss: 0.001909283888685104\n",
      "Epoch 10, Loss: 0.0018236518302878876\n",
      "Epoch 11, Loss: 0.0018404054534044685\n",
      "Epoch 12, Loss: 0.0016993436806448995\n",
      "Epoch 13, Loss: 0.0015590464302106246\n",
      "Epoch 14, Loss: 0.001529084107984038\n",
      "Epoch 15, Loss: 0.0014739534043484573\n",
      "Epoch 16, Loss: 0.001364586760141156\n",
      "Epoch 17, Loss: 0.001387319437820043\n",
      "Epoch 18, Loss: 0.0013031122958558467\n",
      "Epoch 19, Loss: 0.001264527167111662\n",
      "Epoch 20, Loss: 0.00124929905281765\n",
      "Epoch 21, Loss: 0.0012288213255848648\n",
      "Epoch 22, Loss: 0.0011755694900229708\n",
      "Epoch 23, Loss: 0.0013087882970491528\n",
      "Epoch 24, Loss: 0.0012084578396961481\n",
      "Epoch 25, Loss: 0.00123291719484158\n",
      "Epoch 26, Loss: 0.001173220359581511\n",
      "Epoch 27, Loss: 0.0010750012912852706\n",
      "Epoch 28, Loss: 0.0011089243807393516\n",
      "Epoch 29, Loss: 0.0010580301523890807\n",
      "Epoch 30, Loss: 0.0010150086441072276\n",
      "Epoch 31, Loss: 0.0010108066363263494\n",
      "Epoch 32, Loss: 0.0010827459439240678\n",
      "Epoch 33, Loss: 0.0011002892556414678\n",
      "Epoch 34, Loss: 0.0011234413626138957\n",
      "Epoch 35, Loss: 0.001007861834390614\n",
      "Epoch 36, Loss: 0.0010157394185503427\n",
      "Epoch 37, Loss: 0.0010429608343416553\n",
      "Epoch 38, Loss: 0.0010023471468573026\n",
      "Epoch 39, Loss: 0.0010016388532996538\n",
      "After interaction 2, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.006650644894450831\n",
      "Epoch 1, Loss: 0.001130791143602963\n",
      "Epoch 2, Loss: 0.0007788460855953639\n",
      "Epoch 3, Loss: 0.0007599722288971746\n",
      "Epoch 4, Loss: 0.0007164526043961153\n",
      "Epoch 5, Loss: 0.0007015481265745345\n",
      "Epoch 6, Loss: 0.0007130719494517604\n",
      "Epoch 7, Loss: 0.0007560649568116979\n",
      "Epoch 8, Loss: 0.0007669357473023213\n",
      "Epoch 9, Loss: 0.0012429584073479682\n",
      "Epoch 10, Loss: 0.0008111478176316546\n",
      "Epoch 11, Loss: 0.0007697595864572734\n",
      "Epoch 12, Loss: 0.0007744417146729371\n",
      "Epoch 13, Loss: 0.0007053970991678233\n",
      "Epoch 14, Loss: 0.0006776778835755792\n",
      "Epoch 15, Loss: 0.0007060437012564479\n",
      "Epoch 16, Loss: 0.0007305021557349387\n",
      "Epoch 17, Loss: 0.0006980839557330268\n",
      "Epoch 18, Loss: 0.0007742327561079185\n",
      "Epoch 19, Loss: 0.000835792293758858\n",
      "Epoch 20, Loss: 0.0007854095720226072\n",
      "Epoch 21, Loss: 0.0007290034308714815\n",
      "Epoch 22, Loss: 0.0007458503947712298\n",
      "Epoch 23, Loss: 0.0007670113353765046\n",
      "Epoch 24, Loss: 0.0006818787039494758\n",
      "Epoch 25, Loss: 0.0007149922491586157\n",
      "Epoch 26, Loss: 0.0007204028809938671\n",
      "Epoch 27, Loss: 0.000713500309760155\n",
      "Epoch 28, Loss: 0.0006817046835746991\n",
      "Epoch 29, Loss: 0.0006620008713909875\n",
      "Epoch 30, Loss: 0.0006356850634468598\n",
      "Epoch 31, Loss: 0.0006021466717446112\n",
      "Epoch 32, Loss: 0.0006342866524806365\n",
      "Epoch 33, Loss: 0.0006238779827073926\n",
      "Epoch 34, Loss: 0.0007016872116715131\n",
      "Epoch 35, Loss: 0.0008603219162934988\n",
      "Epoch 36, Loss: 0.0007032162611601808\n",
      "Epoch 37, Loss: 0.0006942728300576035\n",
      "Epoch 38, Loss: 0.0007055168372651126\n",
      "Epoch 39, Loss: 0.0006706188366586725\n",
      "After interaction 3, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0030642589811566926\n",
      "Epoch 1, Loss: 0.0006216900789573521\n",
      "Epoch 2, Loss: 0.0005245339634417695\n",
      "Epoch 3, Loss: 0.0005385078234491387\n",
      "Epoch 4, Loss: 0.0005617562407735706\n",
      "Epoch 5, Loss: 0.0005610940430201947\n",
      "Epoch 6, Loss: 0.0006266434362404884\n",
      "Epoch 7, Loss: 0.0006624479556197471\n",
      "Epoch 8, Loss: 0.000581038254693787\n",
      "Epoch 9, Loss: 0.0006447606372564644\n",
      "Epoch 10, Loss: 0.0006958877404146653\n",
      "Epoch 11, Loss: 0.0007242295156068652\n",
      "Epoch 12, Loss: 0.000628922180352691\n",
      "Epoch 13, Loss: 0.0006405603650730695\n",
      "Epoch 14, Loss: 0.000684235284355536\n",
      "Epoch 15, Loss: 0.0006583305663059389\n",
      "Epoch 16, Loss: 0.0006748919571861759\n",
      "Epoch 17, Loss: 0.0006459330441399516\n",
      "Epoch 18, Loss: 0.000615422888213047\n",
      "Epoch 19, Loss: 0.0006120796733282328\n",
      "Epoch 20, Loss: 0.0006946269656255014\n",
      "Epoch 21, Loss: 0.0007185455678861001\n",
      "Epoch 22, Loss: 0.0006673238945243028\n",
      "Epoch 23, Loss: 0.000662241477878117\n",
      "Epoch 24, Loss: 0.0007082323182162568\n",
      "Epoch 25, Loss: 0.0006905639613898759\n",
      "Epoch 26, Loss: 0.0007236654336776569\n",
      "Epoch 27, Loss: 0.0006933287452771476\n",
      "Epoch 28, Loss: 0.0006792922109357032\n",
      "Epoch 29, Loss: 0.0005777127473641875\n",
      "Epoch 30, Loss: 0.0006933593515538437\n",
      "Epoch 31, Loss: 0.000652387912502952\n",
      "Epoch 32, Loss: 0.0006257287582483157\n",
      "Epoch 33, Loss: 0.0006225251419607473\n",
      "Epoch 34, Loss: 0.0006497646815026558\n",
      "Epoch 35, Loss: 0.000623995761654832\n",
      "Epoch 36, Loss: 0.0005976234813758409\n",
      "Epoch 37, Loss: 0.0006034301424433479\n",
      "Epoch 38, Loss: 0.0006813226012938431\n",
      "Epoch 39, Loss: 0.0006716246117207787\n",
      "After interaction 4, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0024587421144119884\n",
      "Epoch 1, Loss: 0.0004574680136399607\n",
      "Epoch 2, Loss: 0.00043463952707659435\n",
      "Epoch 3, Loss: 0.0004409049704814106\n",
      "Epoch 4, Loss: 0.00046031793582132784\n",
      "Epoch 5, Loss: 0.0004969762284107174\n",
      "Epoch 6, Loss: 0.0005156435027060645\n",
      "Epoch 7, Loss: 0.0005580824320810516\n",
      "Epoch 8, Loss: 0.0004969429456109269\n",
      "Epoch 9, Loss: 0.0005487395161138126\n",
      "Epoch 10, Loss: 0.0005292659023037957\n",
      "Epoch 11, Loss: 0.0005608263210423126\n",
      "Epoch 12, Loss: 0.0005508083007189836\n",
      "Epoch 13, Loss: 0.0006392166958024999\n",
      "Epoch 14, Loss: 0.0006298467845286174\n",
      "Epoch 15, Loss: 0.0006023326414786467\n",
      "Epoch 16, Loss: 0.0006474281635030816\n",
      "Epoch 17, Loss: 0.0007355211302939648\n",
      "Epoch 18, Loss: 0.0005614650598606529\n",
      "Epoch 19, Loss: 0.0005773137791200357\n",
      "Epoch 20, Loss: 0.0006382205099150556\n",
      "Epoch 21, Loss: 0.0005743446270734718\n",
      "Epoch 22, Loss: 0.0005782693399034494\n",
      "Epoch 23, Loss: 0.000593222546237099\n",
      "Epoch 24, Loss: 0.0006025675860911099\n",
      "Epoch 25, Loss: 0.0006081214563244479\n",
      "Epoch 26, Loss: 0.0005978550159809554\n",
      "Epoch 27, Loss: 0.0006190306775869147\n",
      "Epoch 28, Loss: 0.0006308378585509733\n",
      "Epoch 29, Loss: 0.0006237767897149492\n",
      "Epoch 30, Loss: 0.0006101709198417095\n",
      "Epoch 31, Loss: 0.0005255782688080252\n",
      "Epoch 32, Loss: 0.0005194236478418147\n",
      "Epoch 33, Loss: 0.0005221790981091905\n",
      "Epoch 34, Loss: 0.0005233026164277344\n",
      "Epoch 35, Loss: 0.0005229697109751667\n",
      "Epoch 36, Loss: 0.0005579316977860411\n",
      "Epoch 37, Loss: 0.0005363188899667199\n",
      "Epoch 38, Loss: 0.0005739334165851753\n",
      "Epoch 39, Loss: 0.0006149996155660684\n",
      "After interaction 5, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0017924277639795787\n",
      "Epoch 1, Loss: 0.00045519756762055815\n",
      "Epoch 2, Loss: 0.00045362087095438485\n",
      "Epoch 3, Loss: 0.00047435980446494533\n",
      "Epoch 4, Loss: 0.0005053546254625899\n",
      "Epoch 5, Loss: 0.000502410252731388\n",
      "Epoch 6, Loss: 0.0005169047013948785\n",
      "Epoch 7, Loss: 0.0005482774866939502\n",
      "Epoch 8, Loss: 0.0005566932070397817\n",
      "Epoch 9, Loss: 0.0005921025992990868\n",
      "Epoch 10, Loss: 0.0006263256473163014\n",
      "Epoch 11, Loss: 0.0006454486620404784\n",
      "Epoch 12, Loss: 0.0006323996620293317\n",
      "Epoch 13, Loss: 0.0006273023444408194\n",
      "Epoch 14, Loss: 0.0006369802513450599\n",
      "Epoch 15, Loss: 0.0006536146325646012\n",
      "Epoch 16, Loss: 0.0006347616327188899\n",
      "Epoch 17, Loss: 0.0005724305947162252\n",
      "Epoch 18, Loss: 0.0005357817208184852\n",
      "Epoch 19, Loss: 0.0005523598600846398\n",
      "Epoch 20, Loss: 0.0006011111160300254\n",
      "Epoch 21, Loss: 0.000575651753664136\n",
      "Epoch 22, Loss: 0.0005889791851102237\n",
      "Epoch 23, Loss: 0.0005416088793258896\n",
      "Epoch 24, Loss: 0.0005519595256199469\n",
      "Epoch 25, Loss: 0.0008035336812697139\n",
      "Epoch 26, Loss: 0.0006498392888936154\n",
      "Epoch 27, Loss: 0.0005799565272504154\n",
      "Epoch 28, Loss: 0.0006187585001799003\n",
      "Epoch 29, Loss: 0.000610756019762918\n",
      "Epoch 30, Loss: 0.0005457571157856528\n",
      "Epoch 31, Loss: 0.0005149728194539767\n",
      "Epoch 32, Loss: 0.0006779365851654664\n",
      "Epoch 33, Loss: 0.0007187644150072062\n",
      "Epoch 34, Loss: 0.0005746122654589339\n",
      "Epoch 35, Loss: 0.0005255238977099128\n",
      "Epoch 36, Loss: 0.0005252262521973968\n",
      "Epoch 37, Loss: 0.0005377047203739715\n",
      "Epoch 38, Loss: 0.0005715953485090975\n",
      "Epoch 39, Loss: 0.0005923504413076393\n",
      "After interaction 6, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0019415726708012413\n",
      "Epoch 1, Loss: 0.0004238985741174663\n",
      "Epoch 2, Loss: 0.00041750471033692504\n",
      "Epoch 3, Loss: 0.0004271730877865373\n",
      "Epoch 4, Loss: 0.00044339499900473544\n",
      "Epoch 5, Loss: 0.00045117192827484414\n",
      "Epoch 6, Loss: 0.0004766216590831816\n",
      "Epoch 7, Loss: 0.00048517153359395153\n",
      "Epoch 8, Loss: 0.000509179857453865\n",
      "Epoch 9, Loss: 0.0005467282839939035\n",
      "Epoch 10, Loss: 0.0005594685459672323\n",
      "Epoch 11, Loss: 0.0005283029997350338\n",
      "Epoch 12, Loss: 0.0004905359150259234\n",
      "Epoch 13, Loss: 0.0005310879265183732\n",
      "Epoch 14, Loss: 0.0005680043090432359\n",
      "Epoch 15, Loss: 0.0005367520216881052\n",
      "Epoch 16, Loss: 0.0005387075857777726\n",
      "Epoch 17, Loss: 0.0004917322274625687\n",
      "Epoch 18, Loss: 0.0005696240154726847\n",
      "Epoch 19, Loss: 0.000566539347576021\n",
      "Epoch 20, Loss: 0.0005411452585811583\n",
      "Epoch 21, Loss: 0.0005110090228762791\n",
      "Epoch 22, Loss: 0.0005287909662559505\n",
      "Epoch 23, Loss: 0.0005670466656235013\n",
      "Epoch 24, Loss: 0.0005407684510912154\n",
      "Epoch 25, Loss: 0.0005933668840410686\n",
      "Epoch 26, Loss: 0.000515237232736711\n",
      "Epoch 27, Loss: 0.0005113459510764992\n",
      "Epoch 28, Loss: 0.0005475608937116873\n",
      "Epoch 29, Loss: 0.0005031170092051179\n",
      "Epoch 30, Loss: 0.0005571552934964137\n",
      "Epoch 31, Loss: 0.0005541532334688246\n",
      "Epoch 32, Loss: 0.0005020636006308494\n",
      "Epoch 33, Loss: 0.0005251564074966188\n",
      "Epoch 34, Loss: 0.0005239389511807476\n",
      "Epoch 35, Loss: 0.0005129007512457032\n",
      "Epoch 36, Loss: 0.0005342523491497056\n",
      "Epoch 37, Loss: 0.0005270578908456342\n",
      "Epoch 38, Loss: 0.0005672180674742535\n",
      "Epoch 39, Loss: 0.000575665215279716\n",
      "After interaction 7, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0013008904418565972\n",
      "Epoch 1, Loss: 0.0003761985148800181\n",
      "Epoch 2, Loss: 0.00038172128480845575\n",
      "Epoch 3, Loss: 0.00041170945451524614\n",
      "Epoch 4, Loss: 0.0004320339723667834\n",
      "Epoch 5, Loss: 0.00042064473629264715\n",
      "Epoch 6, Loss: 0.00048444640649437405\n",
      "Epoch 7, Loss: 0.000493609397560177\n",
      "Epoch 8, Loss: 0.00045907422231194036\n",
      "Epoch 9, Loss: 0.000476580417305914\n",
      "Epoch 10, Loss: 0.0004882761497695167\n",
      "Epoch 11, Loss: 0.0005188808263510121\n",
      "Epoch 12, Loss: 0.0004566793357662011\n",
      "Epoch 13, Loss: 0.000506242461466059\n",
      "Epoch 14, Loss: 0.0004930513349577173\n",
      "Epoch 15, Loss: 0.000499425730365047\n",
      "Epoch 16, Loss: 0.000493778534932862\n",
      "Epoch 17, Loss: 0.00047205914951176163\n",
      "Epoch 18, Loss: 0.0005073596727511527\n",
      "Epoch 19, Loss: 0.0004788214151229878\n",
      "Epoch 20, Loss: 0.00045830106861245\n",
      "Epoch 21, Loss: 0.0004720380324399199\n",
      "Epoch 22, Loss: 0.0004824499167892921\n",
      "Epoch 23, Loss: 0.00046122151941529316\n",
      "Epoch 24, Loss: 0.00047167788425220194\n",
      "Epoch 25, Loss: 0.00048278444248184667\n",
      "Epoch 26, Loss: 0.00045500280557511294\n",
      "Epoch 27, Loss: 0.0004694106804609444\n",
      "Epoch 28, Loss: 0.00047644494955650793\n",
      "Epoch 29, Loss: 0.000490404528956527\n",
      "Epoch 30, Loss: 0.0004411387598225909\n",
      "Epoch 31, Loss: 0.00043947488556140624\n",
      "Epoch 32, Loss: 0.0005017886085372888\n",
      "Epoch 33, Loss: 0.0004623980038090687\n",
      "Epoch 34, Loss: 0.0004636632286879865\n",
      "Epoch 35, Loss: 0.00047427984954614874\n",
      "Epoch 36, Loss: 0.0004710222113185098\n",
      "Epoch 37, Loss: 0.00045280212803312304\n",
      "Epoch 38, Loss: 0.00047573900384902123\n",
      "Epoch 39, Loss: 0.0004557126880306931\n",
      "After interaction 8, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0008418391761624879\n",
      "Epoch 1, Loss: 0.0003511360862344592\n",
      "Epoch 2, Loss: 0.00036916654012496985\n",
      "Epoch 3, Loss: 0.00041327293359234206\n",
      "Epoch 4, Loss: 0.0004655241674132666\n",
      "Epoch 5, Loss: 0.0004049492866371723\n",
      "Epoch 6, Loss: 0.00039431829179280184\n",
      "Epoch 7, Loss: 0.00044058876005919094\n",
      "Epoch 8, Loss: 0.0004789150217160261\n",
      "Epoch 9, Loss: 0.0004371562640158796\n",
      "Epoch 10, Loss: 0.0004268053515561042\n",
      "Epoch 11, Loss: 0.0004350286242929964\n",
      "Epoch 12, Loss: 0.0004164946150946354\n",
      "Epoch 13, Loss: 0.00043150956086475184\n",
      "Epoch 14, Loss: 0.00041752529636635184\n",
      "Epoch 15, Loss: 0.0004204982166649778\n",
      "Epoch 16, Loss: 0.00043821395588196696\n",
      "Epoch 17, Loss: 0.00044841754290221727\n",
      "Epoch 18, Loss: 0.00043416930488262027\n",
      "Epoch 19, Loss: 0.00044685478720914\n",
      "Epoch 20, Loss: 0.00041899322786565174\n",
      "Epoch 21, Loss: 0.00043441444282459247\n",
      "Epoch 22, Loss: 0.000439480807447273\n",
      "Epoch 23, Loss: 0.00044953394986896534\n",
      "Epoch 24, Loss: 0.0004052871336398505\n",
      "Epoch 25, Loss: 0.00045509086117898984\n",
      "Epoch 26, Loss: 0.0004480109531835515\n",
      "Epoch 27, Loss: 0.0004264533068602911\n",
      "Epoch 28, Loss: 0.0004209262333275659\n",
      "Epoch 29, Loss: 0.00041864570472489074\n",
      "Epoch 30, Loss: 0.000477238437444808\n",
      "Epoch 31, Loss: 0.0004437161293762092\n",
      "Epoch 32, Loss: 0.00041297456016503326\n",
      "Epoch 33, Loss: 0.0004362797149400866\n",
      "Epoch 34, Loss: 0.00040988513104720463\n",
      "Epoch 35, Loss: 0.00041242707162662394\n",
      "Epoch 36, Loss: 0.0004384425079703591\n",
      "Epoch 37, Loss: 0.0004157497284697426\n",
      "Epoch 38, Loss: 0.0004071782706367148\n",
      "Epoch 39, Loss: 0.00042361905953735717\n",
      "After interaction 9, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0006449231445705564\n",
      "Epoch 1, Loss: 0.00033866289857928145\n",
      "Epoch 2, Loss: 0.00036798840314227784\n",
      "Epoch 3, Loss: 0.00035827949724426126\n",
      "Epoch 4, Loss: 0.0004148529088870205\n",
      "Epoch 5, Loss: 0.0004094648494253061\n",
      "Epoch 6, Loss: 0.00038664577753629655\n",
      "Epoch 7, Loss: 0.00042944841454670927\n",
      "Epoch 8, Loss: 0.0003963545098561662\n",
      "Epoch 9, Loss: 0.00038551832953506184\n",
      "Epoch 10, Loss: 0.00040838994625784925\n",
      "Epoch 11, Loss: 0.00038063975522688743\n",
      "Epoch 12, Loss: 0.00040181605458300905\n",
      "Epoch 13, Loss: 0.0003938818809094989\n",
      "Epoch 14, Loss: 0.00039984351314312086\n",
      "Epoch 15, Loss: 0.0003974905158497609\n",
      "Epoch 16, Loss: 0.0003988913068619371\n",
      "Epoch 17, Loss: 0.00040824262643514487\n",
      "Epoch 18, Loss: 0.0004334442721235018\n",
      "Epoch 19, Loss: 0.00037439047082115317\n",
      "Epoch 20, Loss: 0.00038726881684452693\n",
      "Epoch 21, Loss: 0.0003877693833802266\n",
      "Epoch 22, Loss: 0.00038056911254062\n",
      "Epoch 23, Loss: 0.00039384764492087653\n",
      "Epoch 24, Loss: 0.00037244174758445636\n",
      "Epoch 25, Loss: 0.00036318623061285665\n",
      "Epoch 26, Loss: 0.0003745571502416752\n",
      "Epoch 27, Loss: 0.000425010714681553\n",
      "Epoch 28, Loss: 0.00039150845535794167\n",
      "Epoch 29, Loss: 0.0004096811390056437\n",
      "Epoch 30, Loss: 0.00039184538041960184\n",
      "Epoch 31, Loss: 0.0003793856094503982\n",
      "Epoch 32, Loss: 0.0003856562455238394\n",
      "Epoch 33, Loss: 0.0003774962311910134\n",
      "Epoch 34, Loss: 0.0004012930773435348\n",
      "Epoch 35, Loss: 0.0003802376011642917\n",
      "Epoch 36, Loss: 0.00038285438098962946\n",
      "Epoch 37, Loss: 0.0003764743146864629\n",
      "Epoch 38, Loss: 0.0003914896049291218\n",
      "Epoch 39, Loss: 0.0003784422379243299\n",
      "After interaction 10, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0005714166529085179\n",
      "Epoch 1, Loss: 0.0003339178986382459\n",
      "Epoch 2, Loss: 0.00035772595814644785\n",
      "Epoch 3, Loss: 0.00036173909641624015\n",
      "Epoch 4, Loss: 0.00038251355287466355\n",
      "Epoch 5, Loss: 0.00037068655949499765\n",
      "Epoch 6, Loss: 0.00038725577679691757\n",
      "Epoch 7, Loss: 0.00038117610303441737\n",
      "Epoch 8, Loss: 0.0003688095318704397\n",
      "Epoch 9, Loss: 0.0003734143441486916\n",
      "Epoch 10, Loss: 0.0003730051899584783\n",
      "Epoch 11, Loss: 0.00038688570854660616\n",
      "Epoch 12, Loss: 0.0003713952853097906\n",
      "Epoch 13, Loss: 0.00037319243532279024\n",
      "Epoch 14, Loss: 0.0003576443780769257\n",
      "Epoch 15, Loss: 0.0003775287245804785\n",
      "Epoch 16, Loss: 0.00039078914103175935\n",
      "Epoch 17, Loss: 0.0003972606034248125\n",
      "Epoch 18, Loss: 0.0003699117993020232\n",
      "Epoch 19, Loss: 0.00037283735818328956\n",
      "Epoch 20, Loss: 0.00036559987234438394\n",
      "Epoch 21, Loss: 0.0003649546498966358\n",
      "Epoch 22, Loss: 0.0003810788607554248\n",
      "Epoch 23, Loss: 0.0003670345276409343\n",
      "Epoch 24, Loss: 0.0003936971798940517\n",
      "Epoch 25, Loss: 0.00037230408879834395\n",
      "Epoch 26, Loss: 0.00035750948589560597\n",
      "Epoch 27, Loss: 0.0003581625701722925\n",
      "Epoch 28, Loss: 0.0003702536613771422\n",
      "Epoch 29, Loss: 0.0003635581642705168\n",
      "Epoch 30, Loss: 0.0003743911536572021\n",
      "Epoch 31, Loss: 0.0003688226730094765\n",
      "Epoch 32, Loss: 0.00039473866785856245\n",
      "Epoch 33, Loss: 0.00035317497166636455\n",
      "Epoch 34, Loss: 0.0003879618207276055\n",
      "Epoch 35, Loss: 0.00036710504185003906\n",
      "Epoch 36, Loss: 0.00037067716517075693\n",
      "Epoch 37, Loss: 0.0003591655127202793\n",
      "Epoch 38, Loss: 0.0003764226595786378\n",
      "Epoch 39, Loss: 0.00036769321883444605\n",
      "After interaction 11, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0004764192392288492\n",
      "Epoch 1, Loss: 0.0003038602416632722\n",
      "Epoch 2, Loss: 0.0003351349996505316\n",
      "Epoch 3, Loss: 0.00033716777674570246\n",
      "Epoch 4, Loss: 0.00032500624518263767\n",
      "Epoch 5, Loss: 0.00033975563371879686\n",
      "Epoch 6, Loss: 0.0003354851813268661\n",
      "Epoch 7, Loss: 0.00033788920649652535\n",
      "Epoch 8, Loss: 0.00033411700932620977\n",
      "Epoch 9, Loss: 0.0003372916097946865\n",
      "Epoch 10, Loss: 0.00033869684993441314\n",
      "Epoch 11, Loss: 0.00034186792494642044\n",
      "Epoch 12, Loss: 0.00032766653134411416\n",
      "Epoch 13, Loss: 0.0003947630801394999\n",
      "Epoch 14, Loss: 0.0003512270838802662\n",
      "Epoch 15, Loss: 0.0003513608224444348\n",
      "Epoch 16, Loss: 0.00032879227380710525\n",
      "Epoch 17, Loss: 0.0003384736346522746\n",
      "Epoch 18, Loss: 0.00032721365072603135\n",
      "Epoch 19, Loss: 0.0003310806075704645\n",
      "Epoch 20, Loss: 0.00035206174792947435\n",
      "Epoch 21, Loss: 0.00032899226400086914\n",
      "Epoch 22, Loss: 0.0003322978810832654\n",
      "Epoch 23, Loss: 0.0003244416815053784\n",
      "Epoch 24, Loss: 0.00032595589089948895\n",
      "Epoch 25, Loss: 0.0003557612025516173\n",
      "Epoch 26, Loss: 0.00032568625911729\n",
      "Epoch 27, Loss: 0.0003496521632777554\n",
      "Epoch 28, Loss: 0.0003594077116254046\n",
      "Epoch 29, Loss: 0.0003294930545756393\n",
      "Epoch 30, Loss: 0.00035368049105876417\n",
      "Epoch 31, Loss: 0.0003307588402598046\n",
      "Epoch 32, Loss: 0.00032656473508564155\n",
      "Epoch 33, Loss: 0.0003312547569268358\n",
      "Epoch 34, Loss: 0.000382359580830198\n",
      "Epoch 35, Loss: 0.0003353132548363016\n",
      "Epoch 36, Loss: 0.00033136074186616176\n",
      "Epoch 37, Loss: 0.00032767983410544844\n",
      "Epoch 38, Loss: 0.00033401589224029446\n",
      "Epoch 39, Loss: 0.00031781585978851186\n",
      "After interaction 12, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0004533390089346825\n",
      "Epoch 1, Loss: 0.0002956267481195744\n",
      "Epoch 2, Loss: 0.00030954232376408997\n",
      "Epoch 3, Loss: 0.00031007004315862307\n",
      "Epoch 4, Loss: 0.0003291258796090345\n",
      "Epoch 5, Loss: 0.00033072036241355273\n",
      "Epoch 6, Loss: 0.0003165865615988282\n",
      "Epoch 7, Loss: 0.0003240721714261332\n",
      "Epoch 8, Loss: 0.0003390240204601947\n",
      "Epoch 9, Loss: 0.00031553382864440895\n",
      "Epoch 10, Loss: 0.00033627104951499575\n",
      "Epoch 11, Loss: 0.00032417603213100323\n",
      "Epoch 12, Loss: 0.0003299775642689331\n",
      "Epoch 13, Loss: 0.00032078711913565284\n",
      "Epoch 14, Loss: 0.00031258207650262227\n",
      "Epoch 15, Loss: 0.0003251316633258493\n",
      "Epoch 16, Loss: 0.00033954724064470907\n",
      "Epoch 17, Loss: 0.00032077875419193674\n",
      "Epoch 18, Loss: 0.00031296958842240685\n",
      "Epoch 19, Loss: 0.0003167230994307579\n",
      "Epoch 20, Loss: 0.0003208087230744817\n",
      "Epoch 21, Loss: 0.00033678971666788654\n",
      "Epoch 22, Loss: 0.00033079779962320757\n",
      "Epoch 23, Loss: 0.0003389398216934701\n",
      "Epoch 24, Loss: 0.00031274854706054894\n",
      "Epoch 25, Loss: 0.00032456660934652363\n",
      "Epoch 26, Loss: 0.0003093917781020025\n",
      "Epoch 27, Loss: 0.0003206748505328494\n",
      "Epoch 28, Loss: 0.0003179827572767484\n",
      "Epoch 29, Loss: 0.00031698865715254764\n",
      "Epoch 30, Loss: 0.00031459008061941285\n",
      "Epoch 31, Loss: 0.00032822926922416963\n",
      "Epoch 32, Loss: 0.00030832810969109276\n",
      "Epoch 33, Loss: 0.0003155174158094384\n",
      "Epoch 34, Loss: 0.00031839052099423874\n",
      "Epoch 35, Loss: 0.00032407662183735116\n",
      "Epoch 36, Loss: 0.0003113374654549014\n",
      "Epoch 37, Loss: 0.0003074036026155968\n",
      "Epoch 38, Loss: 0.00032338436969696295\n",
      "Epoch 39, Loss: 0.0003261642359225404\n",
      "After interaction 13, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.00038733169734330873\n",
      "Epoch 1, Loss: 0.0002811521915052905\n",
      "Epoch 2, Loss: 0.0003020216140588692\n",
      "Epoch 3, Loss: 0.0002971682196307526\n",
      "Epoch 4, Loss: 0.0002968369798353469\n",
      "Epoch 5, Loss: 0.0002968004192698462\n",
      "Epoch 6, Loss: 0.00030746014309048484\n",
      "Epoch 7, Loss: 0.00029654775385808607\n",
      "Epoch 8, Loss: 0.00031308208401987516\n",
      "Epoch 9, Loss: 0.00031741419035707577\n",
      "Epoch 10, Loss: 0.00029975815445973364\n",
      "Epoch 11, Loss: 0.000301065583305732\n",
      "Epoch 12, Loss: 0.0003053718616389445\n",
      "Epoch 13, Loss: 0.0002971651666362014\n",
      "Epoch 14, Loss: 0.00030599827726897447\n",
      "Epoch 15, Loss: 0.0003010401461633139\n",
      "Epoch 16, Loss: 0.0003069046849733626\n",
      "Epoch 17, Loss: 0.00031041717123003715\n",
      "Epoch 18, Loss: 0.00030121496144894076\n",
      "Epoch 19, Loss: 0.00030388816611327097\n",
      "Epoch 20, Loss: 0.000306783856594044\n",
      "Epoch 21, Loss: 0.0003018887580726501\n",
      "Epoch 22, Loss: 0.00029619796753079035\n",
      "Epoch 23, Loss: 0.0003126748351463953\n",
      "Epoch 24, Loss: 0.0003030257803070809\n",
      "Epoch 25, Loss: 0.00029810603851112666\n",
      "Epoch 26, Loss: 0.00029757651859748123\n",
      "Epoch 27, Loss: 0.0002945037771988651\n",
      "Epoch 28, Loss: 0.0003108433547576468\n",
      "Epoch 29, Loss: 0.00030299463411669\n",
      "Epoch 30, Loss: 0.0003061007615593296\n",
      "Epoch 31, Loss: 0.0002961469176829187\n",
      "Epoch 32, Loss: 0.0002975287874536072\n",
      "Epoch 33, Loss: 0.0002974568195811496\n",
      "Epoch 34, Loss: 0.00030692038338206743\n",
      "Epoch 35, Loss: 0.00030918017556967305\n",
      "Epoch 36, Loss: 0.0002951259747860773\n",
      "Epoch 37, Loss: 0.00029453466983388833\n",
      "Epoch 38, Loss: 0.0002855560534273911\n",
      "Epoch 39, Loss: 0.00029920940935014285\n",
      "After interaction 14, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0003508733525032448\n",
      "Epoch 1, Loss: 0.000286713063838396\n",
      "Epoch 2, Loss: 0.0003010328633743706\n",
      "Epoch 3, Loss: 0.0003023673829181453\n",
      "Epoch 4, Loss: 0.00030034451165709746\n",
      "Epoch 5, Loss: 0.00030137064881731045\n",
      "Epoch 6, Loss: 0.0003075217895932724\n",
      "Epoch 7, Loss: 0.00033604884704075663\n",
      "Epoch 8, Loss: 0.0002915051728023578\n",
      "Epoch 9, Loss: 0.0003002812176021585\n",
      "Epoch 10, Loss: 0.0003104717584965688\n",
      "Epoch 11, Loss: 0.000300277400331005\n",
      "Epoch 12, Loss: 0.00029825659511609534\n",
      "Epoch 13, Loss: 0.0003087289381941219\n",
      "Epoch 14, Loss: 0.00031704092385494745\n",
      "Epoch 15, Loss: 0.00029766770418149794\n",
      "Epoch 16, Loss: 0.00030716298580142095\n",
      "Epoch 17, Loss: 0.00030522990472269424\n",
      "Epoch 18, Loss: 0.0002995288155514068\n",
      "Epoch 19, Loss: 0.00030245779595481904\n",
      "Epoch 20, Loss: 0.0002992675315894243\n",
      "Epoch 21, Loss: 0.000311776769302728\n",
      "Epoch 22, Loss: 0.0003021381385565801\n",
      "Epoch 23, Loss: 0.00030241565289990195\n",
      "Epoch 24, Loss: 0.0002944184568528811\n",
      "Epoch 25, Loss: 0.0002926292954829505\n",
      "Epoch 26, Loss: 0.0002962882999142396\n",
      "Epoch 27, Loss: 0.0003043987310549245\n",
      "Epoch 28, Loss: 0.0003093782471157834\n",
      "Epoch 29, Loss: 0.00030013952899853415\n",
      "Epoch 30, Loss: 0.000310302432436668\n",
      "Epoch 31, Loss: 0.00029752233760313315\n",
      "Epoch 32, Loss: 0.0003058946878653927\n",
      "Epoch 33, Loss: 0.00030113326412991865\n",
      "Epoch 34, Loss: 0.0002912590087045409\n",
      "Epoch 35, Loss: 0.0003046284566625887\n",
      "Epoch 36, Loss: 0.00029746762195546173\n",
      "Epoch 37, Loss: 0.0002930959906325767\n",
      "Epoch 38, Loss: 0.0002987775563973686\n",
      "Epoch 39, Loss: 0.00029747061552865803\n",
      "After interaction 15, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.0003193089518953105\n",
      "Epoch 1, Loss: 0.00026993719153508726\n",
      "Epoch 2, Loss: 0.0002816853304381904\n",
      "Epoch 3, Loss: 0.00028726048597137454\n",
      "Epoch 4, Loss: 0.00028695616214800915\n",
      "Epoch 5, Loss: 0.0002950350908208384\n",
      "Epoch 6, Loss: 0.00029191403336275207\n",
      "Epoch 7, Loss: 0.00030957182720097733\n",
      "Epoch 8, Loss: 0.00030550549079740247\n",
      "Epoch 9, Loss: 0.0002862648310046862\n",
      "Epoch 10, Loss: 0.0002873141920554241\n",
      "Epoch 11, Loss: 0.00028779269116631427\n",
      "Epoch 12, Loss: 0.00028859664235749035\n",
      "Epoch 13, Loss: 0.0002832990479361428\n",
      "Epoch 14, Loss: 0.0003044420966336887\n",
      "Epoch 15, Loss: 0.00028576550151813117\n",
      "Epoch 16, Loss: 0.00029074858946269834\n",
      "Epoch 17, Loss: 0.000289873423008491\n",
      "Epoch 18, Loss: 0.00027832893888792676\n",
      "Epoch 19, Loss: 0.00028633075526645154\n",
      "Epoch 20, Loss: 0.0002884402760588213\n",
      "Epoch 21, Loss: 0.000288698124495552\n",
      "Epoch 22, Loss: 0.000284308784054819\n",
      "Epoch 23, Loss: 0.00028631800837581234\n",
      "Epoch 24, Loss: 0.0002930165615130278\n",
      "Epoch 25, Loss: 0.00028106095223240493\n",
      "Epoch 26, Loss: 0.00028849844892118385\n",
      "Epoch 27, Loss: 0.0002942874646813233\n",
      "Epoch 28, Loss: 0.00028528675502877447\n",
      "Epoch 29, Loss: 0.0002802989157079124\n",
      "Epoch 30, Loss: 0.0002969429473481056\n",
      "Epoch 31, Loss: 0.000282532982054997\n",
      "Epoch 32, Loss: 0.0002906271895687656\n",
      "Epoch 33, Loss: 0.0002689108316369158\n",
      "Epoch 34, Loss: 0.00028260269651837413\n",
      "Epoch 35, Loss: 0.00027902335940417487\n",
      "Epoch 36, Loss: 0.00028071087771713274\n",
      "Epoch 37, Loss: 0.0002787873886129775\n",
      "Epoch 38, Loss: 0.0002831871524825966\n",
      "Epoch 39, Loss: 0.0002936191978931492\n",
      "After interaction 16, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.00030327657734131994\n",
      "Epoch 1, Loss: 0.00025784321648360107\n",
      "Epoch 2, Loss: 0.0002625276142224797\n",
      "Epoch 3, Loss: 0.0002771566544191026\n",
      "Epoch 4, Loss: 0.0002793094723291742\n",
      "Epoch 5, Loss: 0.00027133282150791903\n",
      "Epoch 6, Loss: 0.000270483922114684\n",
      "Epoch 7, Loss: 0.0002688854439145446\n",
      "Epoch 8, Loss: 0.00027475501433795726\n",
      "Epoch 9, Loss: 0.00027077998546570683\n",
      "Epoch 10, Loss: 0.0002789385252128414\n",
      "Epoch 11, Loss: 0.00028771228174601354\n",
      "Epoch 12, Loss: 0.0002823027974479651\n",
      "Epoch 13, Loss: 0.0002745671620794987\n",
      "Epoch 14, Loss: 0.0002707645819017871\n",
      "Epoch 15, Loss: 0.0002773361281360212\n",
      "Epoch 16, Loss: 0.00026532954189179106\n",
      "Epoch 17, Loss: 0.0002792174030669057\n",
      "Epoch 18, Loss: 0.0002665781321033842\n",
      "Epoch 19, Loss: 0.00026795056788589053\n",
      "Epoch 20, Loss: 0.0002650557311818837\n",
      "Epoch 21, Loss: 0.00026753941553140433\n",
      "Epoch 22, Loss: 0.0002773670774569243\n",
      "Epoch 23, Loss: 0.00026618005599358924\n",
      "Epoch 24, Loss: 0.000281621441737676\n",
      "Epoch 25, Loss: 0.0002751620898381806\n",
      "Epoch 26, Loss: 0.0002880930259932549\n",
      "Epoch 27, Loss: 0.00027798550429668136\n",
      "Epoch 28, Loss: 0.0002689830948017412\n",
      "Epoch 29, Loss: 0.00026601323984087215\n",
      "Epoch 30, Loss: 0.0002621812952614536\n",
      "Epoch 31, Loss: 0.00027179353341976717\n",
      "Epoch 32, Loss: 0.0002643228723047656\n",
      "Epoch 33, Loss: 0.00026777359943919697\n",
      "Epoch 34, Loss: 0.00026681258697861974\n",
      "Epoch 35, Loss: 0.0002767273276891189\n",
      "Epoch 36, Loss: 0.00026791069717258773\n",
      "Epoch 37, Loss: 0.00026970653318612696\n",
      "Epoch 38, Loss: 0.0002710015061890538\n",
      "Epoch 39, Loss: 0.00027725008092618285\n",
      "After interaction 17, reward = 285.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.00029784757094671364\n",
      "Epoch 1, Loss: 0.0002665639667937142\n",
      "Epoch 2, Loss: 0.00027140224870707714\n",
      "Epoch 3, Loss: 0.0002978632172643242\n",
      "Epoch 4, Loss: 0.0002769549420944067\n",
      "Epoch 5, Loss: 0.00027810488987560164\n",
      "Epoch 6, Loss: 0.00027470676698595293\n",
      "Epoch 7, Loss: 0.00027066056664333624\n",
      "Epoch 8, Loss: 0.0002711997736835565\n",
      "Epoch 9, Loss: 0.0002759128252643704\n",
      "Epoch 10, Loss: 0.0002784140503353942\n",
      "Epoch 11, Loss: 0.00027483611430167396\n",
      "Epoch 12, Loss: 0.0002784343979474225\n",
      "Epoch 13, Loss: 0.0002849645161611923\n",
      "Epoch 14, Loss: 0.0002794540881038198\n",
      "Epoch 15, Loss: 0.00026818053709628484\n",
      "Epoch 16, Loss: 0.0002745016078744769\n",
      "Epoch 17, Loss: 0.00027367171088528197\n",
      "Epoch 18, Loss: 0.00027077962176184174\n",
      "Epoch 19, Loss: 0.00028146226996794737\n",
      "Epoch 20, Loss: 0.00026891776574568493\n",
      "Epoch 21, Loss: 0.0002761813649393305\n",
      "Epoch 22, Loss: 0.00027564832051299736\n",
      "Epoch 23, Loss: 0.00027974795041359196\n",
      "Epoch 24, Loss: 0.0002738058545097034\n",
      "Epoch 25, Loss: 0.0002702284004827958\n",
      "Epoch 26, Loss: 0.00027594383087079633\n",
      "Epoch 27, Loss: 0.0002726875412071905\n",
      "Epoch 28, Loss: 0.000277665863094596\n",
      "Epoch 29, Loss: 0.00028849611551131114\n",
      "Epoch 30, Loss: 0.00027000131840288307\n",
      "Epoch 31, Loss: 0.0002790979283121017\n",
      "Epoch 32, Loss: 0.00027269481992128996\n",
      "Epoch 33, Loss: 0.000269249336693208\n",
      "Epoch 34, Loss: 0.00026536348123141655\n",
      "Epoch 35, Loss: 0.00027192649663880763\n",
      "Epoch 36, Loss: 0.00027552354674098567\n",
      "Epoch 37, Loss: 0.0002671808214534668\n",
      "Epoch 38, Loss: 0.0002663869800075012\n",
      "Epoch 39, Loss: 0.00026608813293929346\n",
      "After interaction 18, reward = 285.0\n",
      "Training the learner\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import dagger\n",
    "\n",
    "dagger.interact(env, learner, agent, observations=[], actions=[], checkpoint_path=\"models/DAgger.pth\", seed=seed, num_epochs=40, tqdm_disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.25\n"
     ]
    }
   ],
   "source": [
    "learner.load_state_dict(torch.load(\"models/DAgger.pth\"), strict=True)\n",
    "total_learner_reward = 0\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(20):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_learner_reward += reward\n",
    "\n",
    "print(total_learner_reward/20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
