{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import AtariDataset\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reseed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "seed = 42\n",
    "reseed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[1960, 1870, 1770, 1705, 1700, 1685, 1665, 1660, 1660, 1605, 1605, 1580, 1525, 1490, 1470]\n"
     ]
    }
   ],
   "source": [
    "dataloader = AtariDataset(\"atari_v1\", 15)\n",
    "observations, actions, rewards, next_observations, dones = dataloader.compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(210, 160)\n",
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_id, seed=25):\n",
    "    env = gym.make(env_id, obs_type='grayscale', render_mode='rgb_array', repeat_action_probability=0.15,frameskip=1)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env\n",
    "env = make_env(\"SpaceInvaders-v0\", seed=seed)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import DQN\n",
    "import dqn\n",
    "\n",
    "\n",
    "def visualize(learner, env, video_name=\"test\"):\n",
    "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
    "\n",
    "        Args:\n",
    "            algorithm (PolicyGradient): Algorithm whose policy network will be rolled out for the episode. If\n",
    "            no algorithm is passed in, a random policy will be visualized.\n",
    "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
    "            when running on local machine.\n",
    "    \"\"\"\n",
    "\n",
    "    import cv2\n",
    "\n",
    "    print(\"Visualizing\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video = cv2.VideoWriter(f\"{video_name}.avi\", fourcc, 24, (160,210), isColor = True)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if isinstance(learner, DQN):\n",
    "            with torch.no_grad():\n",
    "              action = learner.get_action(\n",
    "              torch.tensor(obs).unsqueeze(0), eps=0.0\n",
    "              )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "              action = learner.get_action(\n",
    "              torch.tensor(obs).unsqueeze(0)\n",
    "              )\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        im = env.render(mode='rgb_array')\n",
    "        \n",
    "        video.write(im)\n",
    "\n",
    "    video.release()\n",
    "    env.close()\n",
    "    print(f\"Video saved as {video_name}.avi\")\n",
    "    print(\"Reward: \" + str(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN DQN (TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\jonon\\Documents\\Robot Learning\\CS4756_FinalProj_SpaceInvader\\dqn.py:175: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  val_action = network.get_action(torch.Tensor([val_obs]).to(device), eps=0.00)\n",
      "  4%|▍         | 1/25 [00:52<21:11, 52.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum:  1570.3909033714863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [01:39<18:55, 49.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum:  679.7369761247455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [02:20<16:35, 45.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum:  524.0243489350752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [04:34<14:00, 44.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum:  515.9805828436259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [05:18<13:16, 44.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum:  435.5177795996826\n"
     ]
    }
   ],
   "source": [
    "from dqn import DQN\n",
    "import dqn\n",
    "\n",
    "INPUT_SHAPE = 210*160\n",
    "ACTION_SIZE = env.action_space.n\n",
    "\n",
    "dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(dqn_learner, env, observations=observations, actions=actions, rewards=rewards, next_observations=next_observations, dones=dones, save_path='models/dqn_test.pth', num_episodes=25, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing\n",
      "Video saved as dqn_learner.avi\n",
      "Reward: 205.0\n"
     ]
    }
   ],
   "source": [
    "visualize(dqn_learner, env, \"dqn_learner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the learner\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m learner \u001b[38;5;241m=\u001b[39m SpaceInvLearner(env)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/bc_learner.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonon\\Documents\\Robot Learning\\CS4756_FinalProj_SpaceInvader\\bc.py:43\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(learner, observations, actions, checkpoint_path, num_epochs, tqdm_disable)\u001b[0m\n\u001b[0;32m     41\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     44\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(actions), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[0;32m     45\u001b[0m )  \u001b[38;5;66;03m# Create your dataset\u001b[39;00m\n\u001b[0;32m     46\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     47\u001b[0m     dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     48\u001b[0m )  \u001b[38;5;66;03m# Create your dataloader\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.21 GiB. GPU 0 has a total capacity of 4.00 GiB of which 2.98 GiB is free. Of the allocated memory 181.35 MiB is allocated by PyTorch, and 38.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bc import SpaceInvLearner\n",
    "import bc\n",
    "\n",
    "learner = SpaceInvLearner(env)\n",
    "\n",
    "bc.train(learner=learner, observations=observations, checkpoint_path=\"models/bc_learner.pth\", actions=actions, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonon\\AppData\\Local\\Temp\\ipykernel_21280\\1553395103.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  action = learner.get_action(torch.Tensor([obs]).to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as bc_learner.avi\n",
      "Reward: 80.0\n"
     ]
    }
   ],
   "source": [
    "learner.load_state_dict(torch.load(\"models/bc_learner.pth\"), strict=True)\n",
    "total_learner_reward = 0\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(20):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_learner_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(total_learner_reward/20)\n",
    "\n",
    "visualize(learner, env, \"bc_learner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD EXPERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expert.ppo import PPOAgent, ActorCnn, CriticCnn\n",
    "\n",
    "INPUT_SHAPE = (4, 84, 84)\n",
    "ACTION_SIZE = env.action_space.n\n",
    "SEED = 0\n",
    "GAMMA = 0.99           # discount factor\n",
    "ALPHA= 0.00001         # Actor learning rate\n",
    "BETA = 0.00001          # Critic learning rate\n",
    "TAU = 0.95\n",
    "BATCH_SIZE = 64\n",
    "PPO_EPOCH = 10\n",
    "CLIP_PARAM = 0.2\n",
    "UPDATE_EVERY = 1000    # how often to update the network \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = PPOAgent(INPUT_SHAPE, ACTION_SIZE, SEED, device, GAMMA, ALPHA, BETA, TAU, UPDATE_EVERY, BATCH_SIZE, PPO_EPOCH, CLIP_PARAM, ActorCnn(INPUT_SHAPE, ACTION_SIZE), CriticCnn(INPUT_SHAPE))\n",
    "agent.load_model(\"models/expert_actor.pth\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAgger Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After interaction 0, reward = 80.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.2285896447943706\n",
      "Epoch 1, Loss: 0.16880559284313051\n",
      "Epoch 2, Loss: 0.15796418084817773\n",
      "Epoch 3, Loss: 0.15377965417562747\n",
      "Epoch 4, Loss: 0.1483202627476524\n",
      "Epoch 5, Loss: 0.14525875747203826\n",
      "Epoch 6, Loss: 0.14375673450675666\n",
      "Epoch 7, Loss: 0.14360926916786268\n",
      "Epoch 8, Loss: 0.14317754314226264\n",
      "Epoch 9, Loss: 0.14113674181349137\n",
      "Epoch 10, Loss: 0.13915853751640694\n",
      "Epoch 11, Loss: 0.13925927380720773\n",
      "Epoch 12, Loss: 0.13707467609760807\n",
      "Epoch 13, Loss: 0.13588255976929384\n",
      "Epoch 14, Loss: 0.13434492244439966\n",
      "Epoch 15, Loss: 0.13368839709197775\n",
      "Epoch 16, Loss: 0.1337788970447054\n",
      "Epoch 17, Loss: 0.13292337802110935\n",
      "Epoch 18, Loss: 0.1343384880061243\n",
      "Epoch 19, Loss: 0.13505621949831645\n",
      "Epoch 20, Loss: 0.13200805456030604\n",
      "Epoch 21, Loss: 0.13258715593347362\n",
      "Epoch 22, Loss: 0.1319811149555094\n",
      "Epoch 23, Loss: 0.13122459562385783\n",
      "Epoch 24, Loss: 0.12987088315627154\n",
      "Epoch 25, Loss: 0.12836752119017583\n",
      "Epoch 26, Loss: 0.12998022518905938\n",
      "Epoch 27, Loss: 0.12930921228492961\n",
      "Epoch 28, Loss: 0.1283974100561703\n",
      "Epoch 29, Loss: 0.12880458130555994\n",
      "Epoch 30, Loss: 0.1280671307853624\n",
      "Epoch 31, Loss: 0.12630715492893668\n",
      "Epoch 32, Loss: 0.12653657379103642\n",
      "Epoch 33, Loss: 0.12653904995497536\n",
      "Epoch 34, Loss: 0.12556373535418042\n",
      "Epoch 35, Loss: 0.1255358306800618\n",
      "Epoch 36, Loss: 0.12547819988400327\n",
      "Epoch 37, Loss: 0.12339509532732122\n",
      "Epoch 38, Loss: 0.12361574050258188\n",
      "Epoch 39, Loss: 0.1246303239289452\n",
      "After interaction 1, reward = 105.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.16789906773035004\n",
      "Epoch 1, Loss: 0.145559108563187\n",
      "Epoch 2, Loss: 0.1414777019247227\n",
      "Epoch 3, Loss: 0.1385998357853963\n",
      "Epoch 4, Loss: 0.1372994421858305\n",
      "Epoch 5, Loss: 0.13789188091174012\n",
      "Epoch 6, Loss: 0.137753368270415\n",
      "Epoch 7, Loss: 0.13475630112738532\n",
      "Epoch 8, Loss: 0.13326103357188904\n",
      "Epoch 9, Loss: 0.13255740940481736\n",
      "Epoch 10, Loss: 0.13290105147568457\n",
      "Epoch 11, Loss: 0.13171158776273761\n",
      "Epoch 12, Loss: 0.13030329618396957\n",
      "Epoch 13, Loss: 0.13018422843688865\n",
      "Epoch 14, Loss: 0.13034632198390048\n",
      "Epoch 15, Loss: 0.12991876734627616\n",
      "Epoch 16, Loss: 0.1286571936623398\n",
      "Epoch 17, Loss: 0.1288777337436006\n",
      "Epoch 18, Loss: 0.12968318375207086\n",
      "Epoch 19, Loss: 0.1296923430511235\n",
      "Epoch 20, Loss: 0.1279225748052749\n",
      "Epoch 21, Loss: 0.12634387950161885\n",
      "Epoch 22, Loss: 0.12690430327203064\n",
      "Epoch 23, Loss: 0.12596612836837057\n",
      "Epoch 24, Loss: 0.12440917947931437\n",
      "Epoch 25, Loss: 0.12470313300451474\n",
      "Epoch 26, Loss: 0.12487060112268936\n",
      "Epoch 27, Loss: 0.12485762731941054\n",
      "Epoch 28, Loss: 0.1252264792297275\n",
      "Epoch 29, Loss: 0.12355288538356057\n",
      "Epoch 30, Loss: 0.12587332704736037\n",
      "Epoch 31, Loss: 0.12436158337370935\n",
      "Epoch 32, Loss: 0.12227159573714651\n",
      "Epoch 33, Loss: 0.12193938880772749\n",
      "Epoch 34, Loss: 0.12263427821034867\n",
      "Epoch 35, Loss: 0.12391702543088436\n",
      "Epoch 36, Loss: 0.12139675355649553\n",
      "Epoch 37, Loss: 0.1206361213072533\n",
      "Epoch 38, Loss: 0.12148545304518406\n",
      "Epoch 39, Loss: 0.12194976499039674\n",
      "After interaction 2, reward = 75.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.14702211466857962\n",
      "Epoch 1, Loss: 0.1353873403214541\n",
      "Epoch 2, Loss: 0.13329300744085723\n",
      "Epoch 3, Loss: 0.13127175156429707\n",
      "Epoch 4, Loss: 0.12972351961846368\n",
      "Epoch 5, Loss: 0.1283752091304536\n",
      "Epoch 6, Loss: 0.1295395474379277\n",
      "Epoch 7, Loss: 0.12969930776977673\n",
      "Epoch 8, Loss: 0.12736277306213292\n",
      "Epoch 9, Loss: 0.1268345970098853\n",
      "Epoch 10, Loss: 0.12653835221796697\n",
      "Epoch 11, Loss: 0.1259662028416423\n",
      "Epoch 12, Loss: 0.12618822572103286\n",
      "Epoch 13, Loss: 0.12358006572298089\n",
      "Epoch 14, Loss: 0.12414623880071547\n",
      "Epoch 15, Loss: 0.12339406194450056\n",
      "Epoch 16, Loss: 0.12332254534586612\n",
      "Epoch 17, Loss: 0.12398519930138209\n",
      "Epoch 18, Loss: 0.12413827536791178\n",
      "Epoch 19, Loss: 0.12314619649567776\n",
      "Epoch 20, Loss: 0.12311091279495676\n",
      "Epoch 21, Loss: 0.12425047427092703\n",
      "Epoch 22, Loss: 0.12202422176209227\n",
      "Epoch 23, Loss: 0.12265869532329046\n",
      "Epoch 24, Loss: 0.12186992365180216\n",
      "Epoch 25, Loss: 0.12117319265344668\n",
      "Epoch 26, Loss: 0.12133105795005686\n",
      "Epoch 27, Loss: 0.12012466494594683\n",
      "Epoch 28, Loss: 0.11926367745746803\n",
      "Epoch 29, Loss: 0.1188862929032707\n",
      "Epoch 30, Loss: 0.11989324250632794\n",
      "Epoch 31, Loss: 0.11996345667102697\n",
      "Epoch 32, Loss: 0.11988663155326229\n",
      "Epoch 33, Loss: 0.11809081193053168\n",
      "Epoch 34, Loss: 0.11838843973679908\n",
      "Epoch 35, Loss: 0.11786806906810296\n",
      "Epoch 36, Loss: 0.11781963116016034\n",
      "Epoch 37, Loss: 0.11899430153877763\n",
      "Epoch 38, Loss: 0.11794822450418413\n",
      "Epoch 39, Loss: 0.1178025098067349\n",
      "After interaction 3, reward = 160.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.14650711688128384\n",
      "Epoch 1, Loss: 0.1323699987295902\n",
      "Epoch 2, Loss: 0.12994846521001874\n",
      "Epoch 3, Loss: 0.12909800694747406\n",
      "Epoch 4, Loss: 0.12697519813523148\n",
      "Epoch 5, Loss: 0.1254616327809565\n",
      "Epoch 6, Loss: 0.1252012106053757\n",
      "Epoch 7, Loss: 0.12408637616670493\n",
      "Epoch 8, Loss: 0.12367102548931584\n",
      "Epoch 9, Loss: 0.12199016389521686\n",
      "Epoch 10, Loss: 0.12181656220645616\n",
      "Epoch 11, Loss: 0.12154948914592917\n",
      "Epoch 12, Loss: 0.12161258495215213\n",
      "Epoch 13, Loss: 0.12025871037533789\n",
      "Epoch 14, Loss: 0.11942711833751563\n",
      "Epoch 15, Loss: 0.1192140057682991\n",
      "Epoch 16, Loss: 0.11988196634885037\n",
      "Epoch 17, Loss: 0.11903916209033041\n",
      "Epoch 18, Loss: 0.11834011497822675\n",
      "Epoch 19, Loss: 0.11850496539563844\n",
      "Epoch 20, Loss: 0.11771616791233872\n",
      "Epoch 21, Loss: 0.11768152971159328\n",
      "Epoch 22, Loss: 0.11724387854337692\n",
      "Epoch 23, Loss: 0.11633952977982434\n",
      "Epoch 24, Loss: 0.11554254478577411\n",
      "Epoch 25, Loss: 0.11534982535875205\n",
      "Epoch 26, Loss: 0.11475046633770972\n",
      "Epoch 27, Loss: 0.11441804875027049\n",
      "Epoch 28, Loss: 0.11383640562946146\n",
      "Epoch 29, Loss: 0.11395110122182152\n",
      "Epoch 30, Loss: 0.11406256619727972\n",
      "Epoch 31, Loss: 0.11360523398175384\n",
      "Epoch 32, Loss: 0.11298441615971652\n",
      "Epoch 33, Loss: 0.11171576471039743\n",
      "Epoch 34, Loss: 0.11198993456183058\n",
      "Epoch 35, Loss: 0.11227930269458077\n",
      "Epoch 36, Loss: 0.11149526500340665\n",
      "Epoch 37, Loss: 0.11130236224694685\n",
      "Epoch 38, Loss: 0.1108392510901798\n",
      "Epoch 39, Loss: 0.1102444479862849\n",
      "After interaction 4, reward = 50.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.12237817762498547\n",
      "Epoch 1, Loss: 0.11670772919581517\n",
      "Epoch 2, Loss: 0.11567626700907672\n",
      "Epoch 3, Loss: 0.11502727202247404\n",
      "Epoch 4, Loss: 0.11471750634913631\n",
      "Epoch 5, Loss: 0.11489986597327322\n",
      "Epoch 6, Loss: 0.1148667169652774\n",
      "Epoch 7, Loss: 0.11377312086293426\n",
      "Epoch 8, Loss: 0.11447836512565018\n",
      "Epoch 9, Loss: 0.11339445839499\n",
      "Epoch 10, Loss: 0.11283583934740136\n",
      "Epoch 11, Loss: 0.11263191160542313\n",
      "Epoch 12, Loss: 0.11156007019510293\n",
      "Epoch 13, Loss: 0.11182109993616939\n",
      "Epoch 14, Loss: 0.11105660321260628\n",
      "Epoch 15, Loss: 0.1117894071295969\n",
      "Epoch 16, Loss: 0.11145001999160595\n",
      "Epoch 17, Loss: 0.11145854279312212\n",
      "Epoch 18, Loss: 0.11050855058709087\n",
      "Epoch 19, Loss: 0.11019998802458952\n",
      "Epoch 20, Loss: 0.10947636143302085\n",
      "Epoch 21, Loss: 0.10974944317256621\n",
      "Epoch 22, Loss: 0.10903902150572882\n",
      "Epoch 23, Loss: 0.10888383837288257\n",
      "Epoch 24, Loss: 0.10887051339656834\n",
      "Epoch 25, Loss: 0.10845052882570677\n",
      "Epoch 26, Loss: 0.10943221728799746\n",
      "Epoch 27, Loss: 0.10805303844331407\n",
      "Epoch 28, Loss: 0.10793417643257111\n",
      "Epoch 29, Loss: 0.10783298370596378\n",
      "Epoch 30, Loss: 0.10742205745561462\n",
      "Epoch 31, Loss: 0.10748470175147354\n",
      "Epoch 32, Loss: 0.10692829060881512\n",
      "Epoch 33, Loss: 0.10764496062486645\n",
      "Epoch 34, Loss: 0.10653234594629292\n",
      "Epoch 35, Loss: 0.10557048258713059\n",
      "Epoch 36, Loss: 0.10710766941234655\n",
      "Epoch 37, Loss: 0.10643228814191652\n",
      "Epoch 38, Loss: 0.10497077332742989\n",
      "Epoch 39, Loss: 0.10490673796543952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdagger\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/DAgger.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_disable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonon\\Documents\\Robot Learning\\CS4756_FinalProj_SpaceInvader\\dagger.py:21\u001b[0m, in \u001b[0;36minteract\u001b[1;34m(env, learner, expert, observations, actions, checkpoint_path, seed, num_epochs, tqdm_disable)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 21\u001b[0m         action \u001b[38;5;241m=\u001b[39m learner\u001b[38;5;241m.\u001b[39mget_action(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     22\u001b[0m         expert_actions\u001b[38;5;241m.\u001b[39mappend(expert\u001b[38;5;241m.\u001b[39mact(obs_proc)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     23\u001b[0m     episode_observations\u001b[38;5;241m.\u001b[39mappend(obs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dagger\n",
    "\n",
    "dagger.interact(env, learner, agent, observations=[], actions=[], checkpoint_path=\"models/DAgger.pth\", seed=seed, num_epochs=40, tqdm_disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.25\n"
     ]
    }
   ],
   "source": [
    "learner.load_state_dict(torch.load(\"models/DAgger.pth\"), strict=True)\n",
    "total_learner_reward = 0\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(20):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_learner_reward += reward\n",
    "\n",
    "print(total_learner_reward/20)\n",
    "\n",
    "visualize(learner, env, \"dagger_learner.avi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
