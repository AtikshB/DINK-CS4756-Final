{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import AtariDataset\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reseed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "seed = 42\n",
    "reseed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[1960, 1870, 1770, 1705, 1700, 1685, 1665, 1660, 1660, 1605, 1605, 1580, 1525, 1490, 1470]\n"
     ]
    }
   ],
   "source": [
    "dataloader = AtariDataset(\"atari_v1\", 15)\n",
    "observations, actions, rewards, next_observations, dones = dataloader.compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(210, 160)\n",
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_id, seed=25):\n",
    "    env = gym.make(env_id, obs_type='grayscale', render_mode='rgb_array', repeat_action_probability=0.15,frameskip=1)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env\n",
    "env = make_env(\"SpaceInvaders-v0\", seed=seed)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(learner, env, video_name=\"test\"):\n",
    "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
    "\n",
    "        Args:\n",
    "            algorithm (PolicyGradient): Algorithm whose policy network will be rolled out for the episode. If\n",
    "            no algorithm is passed in, a random policy will be visualized.\n",
    "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
    "            when running on local machine.\n",
    "    \"\"\"\n",
    "\n",
    "    import cv2\n",
    "\n",
    "    print(\"Visualizing\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video = cv2.VideoWriter(f\"{video_name}.avi\", fourcc, 24, (160,210), isColor = True)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        im = env.render(mode='rgb_array')\n",
    "        \n",
    "        video.write(im)\n",
    "\n",
    "    video.release()\n",
    "    env.close()\n",
    "    print(f\"Video saved as {video_name}.avi\")\n",
    "    print(\"Reward: \" + str(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bc import SpaceInvLearner\n",
    "import bc\n",
    "\n",
    "learner = SpaceInvLearner(env)\n",
    "\n",
    "# bc.train(learner=learner, observations=observations, checkpoint_path=\"models/bc_learner.pth\", actions=actions, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing\n",
      "Video saved as bc_learner.avi\n",
      "Reward: 120.0\n"
     ]
    }
   ],
   "source": [
    "learner.load_state_dict(torch.load(\"models/bc_learner.pth\"), strict=True)\n",
    "total_learner_reward = 0\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "visualize(learner, env, \"bc_learner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD EXPERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expert.ppo import PPOAgent, ActorCnn, CriticCnn\n",
    "\n",
    "INPUT_SHAPE = (4, 84, 84)\n",
    "ACTION_SIZE = env.action_space.n\n",
    "SEED = 0\n",
    "GAMMA = 0.99           # discount factor\n",
    "ALPHA= 0.00001         # Actor learning rate\n",
    "BETA = 0.00001          # Critic learning rate\n",
    "TAU = 0.95\n",
    "BATCH_SIZE = 64\n",
    "PPO_EPOCH = 10\n",
    "CLIP_PARAM = 0.2\n",
    "UPDATE_EVERY = 1000    # how often to update the network \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = PPOAgent(INPUT_SHAPE, ACTION_SIZE, SEED, device, GAMMA, ALPHA, BETA, TAU, UPDATE_EVERY, BATCH_SIZE, PPO_EPOCH, CLIP_PARAM, ActorCnn(INPUT_SHAPE, ACTION_SIZE), CriticCnn(INPUT_SHAPE))\n",
    "agent.load_model(\"models/expert_actor.pth\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAgger Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After interaction 0, reward = 120.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.04350557899531346\n",
      "Epoch 1, Loss: 0.042649096414284264\n",
      "Epoch 2, Loss: 0.042080336482019934\n",
      "Epoch 3, Loss: 0.041386895192833166\n",
      "Epoch 4, Loss: 0.04097874933073133\n",
      "Epoch 5, Loss: 0.04056245065371453\n",
      "Epoch 6, Loss: 0.03996931577004949\n",
      "Epoch 7, Loss: 0.03974763405149023\n",
      "Epoch 8, Loss: 0.03914307540737914\n",
      "Epoch 9, Loss: 0.03867110315499403\n",
      "Epoch 10, Loss: 0.038284687184440676\n",
      "Epoch 11, Loss: 0.037936118146633724\n",
      "Epoch 12, Loss: 0.03745183003768604\n",
      "Epoch 13, Loss: 0.03703958397493739\n",
      "Epoch 14, Loss: 0.03677545884181205\n",
      "Epoch 15, Loss: 0.03638671566813788\n",
      "Epoch 16, Loss: 0.036022508853681946\n",
      "Epoch 17, Loss: 0.03569208938625805\n",
      "Epoch 18, Loss: 0.03547194669099109\n",
      "Epoch 19, Loss: 0.03531559657425425\n",
      "Epoch 20, Loss: 0.03473373544232266\n",
      "Epoch 21, Loss: 0.03452082235352728\n",
      "Epoch 22, Loss: 0.03418208397797346\n",
      "Epoch 23, Loss: 0.03396151494406617\n",
      "Epoch 24, Loss: 0.03367556526325791\n",
      "After interaction 1, reward = 410.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.03761608800028166\n",
      "Epoch 1, Loss: 0.03699047341460859\n",
      "Epoch 2, Loss: 0.03645075075696512\n",
      "Epoch 3, Loss: 0.03626459247009585\n",
      "Epoch 4, Loss: 0.03588496247340335\n",
      "Epoch 5, Loss: 0.03560107370355183\n",
      "Epoch 6, Loss: 0.03542325444577691\n",
      "Epoch 7, Loss: 0.03512143578214248\n",
      "Epoch 8, Loss: 0.034915936038821455\n",
      "Epoch 9, Loss: 0.034689014457503324\n",
      "Epoch 10, Loss: 0.03440750148387075\n",
      "Epoch 11, Loss: 0.034173323744838\n",
      "Epoch 12, Loss: 0.033998591325057614\n",
      "Epoch 13, Loss: 0.03374434337322564\n",
      "Epoch 14, Loss: 0.03350361193766517\n",
      "Epoch 15, Loss: 0.03336700590507255\n",
      "Epoch 16, Loss: 0.03308842885705907\n",
      "Epoch 17, Loss: 0.03291193077928781\n",
      "Epoch 18, Loss: 0.03265171796294808\n",
      "Epoch 19, Loss: 0.03249203503939685\n",
      "Epoch 20, Loss: 0.0323360905560408\n",
      "Epoch 21, Loss: 0.03214487948894703\n",
      "Epoch 22, Loss: 0.03180291575071536\n",
      "Epoch 23, Loss: 0.03167925272450947\n",
      "Epoch 24, Loss: 0.03143642808217763\n",
      "After interaction 2, reward = 180.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.03327242717075478\n",
      "Epoch 1, Loss: 0.033054538774742584\n",
      "Epoch 2, Loss: 0.032801362826456446\n",
      "Epoch 3, Loss: 0.03268169697188947\n",
      "Epoch 4, Loss: 0.032406345967177376\n",
      "Epoch 5, Loss: 0.03222851139996222\n",
      "Epoch 6, Loss: 0.032038216736858374\n",
      "Epoch 7, Loss: 0.0319712825650588\n",
      "Epoch 8, Loss: 0.03181706862570934\n",
      "Epoch 9, Loss: 0.031535453844660305\n",
      "Epoch 10, Loss: 0.03154533851056098\n",
      "Epoch 11, Loss: 0.03132064813923696\n",
      "Epoch 12, Loss: 0.031137201107482113\n",
      "Epoch 13, Loss: 0.030962018370218454\n",
      "Epoch 14, Loss: 0.03084768437286618\n",
      "Epoch 15, Loss: 0.030632706553949104\n",
      "Epoch 16, Loss: 0.03049128879325872\n",
      "Epoch 17, Loss: 0.03036157337118598\n",
      "Epoch 18, Loss: 0.030265119930676602\n",
      "Epoch 19, Loss: 0.03004980005435895\n",
      "Epoch 20, Loss: 0.03001063642542383\n",
      "Epoch 21, Loss: 0.02977836934874881\n",
      "Epoch 22, Loss: 0.02966649553524017\n",
      "Epoch 23, Loss: 0.029527866877947857\n",
      "Epoch 24, Loss: 0.029278239218850692\n",
      "After interaction 3, reward = 5.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.030422717210962274\n",
      "Epoch 1, Loss: 0.030101625438930767\n",
      "Epoch 2, Loss: 0.029951001730332123\n",
      "Epoch 3, Loss: 0.029843057214154343\n",
      "Epoch 4, Loss: 0.02973853548515557\n",
      "Epoch 5, Loss: 0.029748171771782815\n",
      "Epoch 6, Loss: 0.0294343329475451\n",
      "Epoch 7, Loss: 0.029419236080077956\n",
      "Epoch 8, Loss: 0.029272218955384337\n",
      "Epoch 9, Loss: 0.02918280340587333\n",
      "Epoch 10, Loss: 0.028982628799447612\n",
      "Epoch 11, Loss: 0.028800234242824897\n",
      "Epoch 12, Loss: 0.0287746363694349\n",
      "Epoch 13, Loss: 0.028652650132362352\n",
      "Epoch 14, Loss: 0.028519159884164834\n",
      "Epoch 15, Loss: 0.028338636080436668\n",
      "Epoch 16, Loss: 0.028384895183263564\n",
      "Epoch 17, Loss: 0.028191436710620993\n",
      "Epoch 18, Loss: 0.028121069958502134\n",
      "Epoch 19, Loss: 0.02798723215413676\n",
      "Epoch 20, Loss: 0.027897047502917688\n",
      "Epoch 21, Loss: 0.027735064093356018\n",
      "Epoch 22, Loss: 0.027718783447255892\n",
      "Epoch 23, Loss: 0.027592558712573288\n",
      "Epoch 24, Loss: 0.027417933514944873\n",
      "After interaction 4, reward = 40.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.029013879769037246\n",
      "Epoch 1, Loss: 0.028704579044772607\n",
      "Epoch 2, Loss: 0.028648163217532153\n",
      "Epoch 3, Loss: 0.028576975144763147\n",
      "Epoch 4, Loss: 0.028481611667211384\n",
      "Epoch 5, Loss: 0.02835465933261335\n",
      "Epoch 6, Loss: 0.028269330610439757\n",
      "Epoch 7, Loss: 0.028142866461541\n",
      "Epoch 8, Loss: 0.02804358666391011\n",
      "Epoch 9, Loss: 0.027867681868990385\n",
      "Epoch 10, Loss: 0.027928644185398765\n",
      "Epoch 11, Loss: 0.027758256914047867\n",
      "Epoch 12, Loss: 0.027681391415345917\n",
      "Epoch 13, Loss: 0.027530056832460858\n",
      "Epoch 14, Loss: 0.0274908110186047\n",
      "Epoch 15, Loss: 0.02743867995768847\n",
      "Epoch 16, Loss: 0.027326803075889337\n",
      "Epoch 17, Loss: 0.027081427821047502\n",
      "Epoch 18, Loss: 0.027212912549043053\n",
      "Epoch 19, Loss: 0.02701937959235017\n",
      "Epoch 20, Loss: 0.026984784915132953\n",
      "Epoch 21, Loss: 0.026894601063990123\n",
      "Epoch 22, Loss: 0.026872481924084115\n",
      "Epoch 23, Loss: 0.026612618767044956\n",
      "Epoch 24, Loss: 0.02670128957212751\n",
      "After interaction 5, reward = 115.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.02826728294190091\n",
      "Epoch 1, Loss: 0.027935333581969517\n",
      "Epoch 2, Loss: 0.0279398988001447\n",
      "Epoch 3, Loss: 0.027868498927808762\n",
      "Epoch 4, Loss: 0.027681716051612366\n",
      "Epoch 5, Loss: 0.02775171693210681\n",
      "Epoch 6, Loss: 0.027529244812195125\n",
      "Epoch 7, Loss: 0.027445317362555184\n",
      "Epoch 8, Loss: 0.027367061375641516\n",
      "Epoch 9, Loss: 0.027223060500846618\n",
      "Epoch 10, Loss: 0.02719249341312993\n",
      "Epoch 11, Loss: 0.027158265945125697\n",
      "Epoch 12, Loss: 0.027128168344799714\n",
      "Epoch 13, Loss: 0.02710896155606717\n",
      "Epoch 14, Loss: 0.02701165012884058\n",
      "Epoch 15, Loss: 0.026769321885279033\n",
      "Epoch 16, Loss: 0.026859387685071717\n",
      "Epoch 17, Loss: 0.026731508362892065\n",
      "Epoch 18, Loss: 0.026628315479121183\n",
      "Epoch 19, Loss: 0.026475292009017524\n",
      "Epoch 20, Loss: 0.02657778173411362\n",
      "Epoch 21, Loss: 0.026437352651946397\n",
      "Epoch 22, Loss: 0.02635873235807738\n",
      "Epoch 23, Loss: 0.026312870951297227\n",
      "Epoch 24, Loss: 0.026289028068808788\n",
      "After interaction 6, reward = 115.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.027633087994534403\n",
      "Epoch 1, Loss: 0.027400564349639497\n",
      "Epoch 2, Loss: 0.02731990002497199\n",
      "Epoch 3, Loss: 0.02719357691269385\n",
      "Epoch 4, Loss: 0.02732452364123824\n",
      "Epoch 5, Loss: 0.02708302156130907\n",
      "Epoch 6, Loss: 0.027091486113703888\n",
      "Epoch 7, Loss: 0.02703398848561678\n",
      "Epoch 8, Loss: 0.02685049136734622\n",
      "Epoch 9, Loss: 0.026765425958040517\n",
      "Epoch 10, Loss: 0.02674702873463234\n",
      "Epoch 11, Loss: 0.026764758347226695\n",
      "Epoch 12, Loss: 0.026613960111564045\n",
      "Epoch 13, Loss: 0.02666506118544845\n",
      "Epoch 14, Loss: 0.026519935536589868\n",
      "Epoch 15, Loss: 0.026523602306960208\n",
      "Epoch 16, Loss: 0.026370088957827362\n",
      "Epoch 17, Loss: 0.026305776530611575\n",
      "Epoch 18, Loss: 0.026396851637694035\n",
      "Epoch 19, Loss: 0.026295173510913115\n",
      "Epoch 20, Loss: 0.026183253708296075\n",
      "Epoch 21, Loss: 0.02612782265786861\n",
      "Epoch 22, Loss: 0.026054489653745617\n",
      "Epoch 23, Loss: 0.025946764842188715\n",
      "Epoch 24, Loss: 0.02604109592827001\n",
      "After interaction 7, reward = 230.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.029001836824646052\n",
      "Epoch 1, Loss: 0.028783401957584758\n",
      "Epoch 2, Loss: 0.028616124712846944\n",
      "Epoch 3, Loss: 0.02858821276718793\n",
      "Epoch 4, Loss: 0.02851334031149152\n",
      "Epoch 5, Loss: 0.02836724815178435\n",
      "Epoch 6, Loss: 0.02828647122960172\n",
      "Epoch 7, Loss: 0.028323526172114\n",
      "Epoch 8, Loss: 0.02819365378290029\n",
      "Epoch 9, Loss: 0.028288463361220588\n",
      "Epoch 10, Loss: 0.028010055294555542\n",
      "Epoch 11, Loss: 0.028014438981247752\n",
      "Epoch 12, Loss: 0.028000269023872926\n",
      "Epoch 13, Loss: 0.027868880073479558\n",
      "Epoch 14, Loss: 0.02790267293706485\n",
      "Epoch 15, Loss: 0.027762857759214526\n",
      "Epoch 16, Loss: 0.02775809592797323\n",
      "Epoch 17, Loss: 0.027665284388815322\n",
      "Epoch 18, Loss: 0.027581160429790648\n",
      "Epoch 19, Loss: 0.027601323363267434\n",
      "Epoch 20, Loss: 0.027571736124272102\n",
      "Epoch 21, Loss: 0.027391912711249133\n",
      "Epoch 22, Loss: 0.027434559701569626\n",
      "Epoch 23, Loss: 0.027379002296444677\n",
      "Epoch 24, Loss: 0.027390482541343495\n",
      "After interaction 8, reward = 240.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.02985854649583474\n",
      "Epoch 1, Loss: 0.0296818313410694\n",
      "Epoch 2, Loss: 0.029556169387960594\n",
      "Epoch 3, Loss: 0.029669269866103076\n",
      "Epoch 4, Loss: 0.029439500254342818\n",
      "Epoch 5, Loss: 0.029455454061315056\n",
      "Epoch 6, Loss: 0.02938943650124048\n",
      "Epoch 7, Loss: 0.029303812723875243\n",
      "Epoch 8, Loss: 0.029150711164228505\n",
      "Epoch 9, Loss: 0.02923315298965612\n",
      "Epoch 10, Loss: 0.029129438499214063\n",
      "Epoch 11, Loss: 0.02917055921355886\n",
      "Epoch 12, Loss: 0.029039376833770083\n",
      "Epoch 13, Loss: 0.029026298867552255\n",
      "Epoch 14, Loss: 0.028876035287079806\n",
      "Epoch 15, Loss: 0.028766679801985162\n",
      "Epoch 16, Loss: 0.029023814103170224\n",
      "Epoch 17, Loss: 0.028818658453951696\n",
      "Epoch 18, Loss: 0.028764380538603303\n",
      "Epoch 19, Loss: 0.028768487846474896\n",
      "Epoch 20, Loss: 0.028683162005448643\n",
      "Epoch 21, Loss: 0.02864881783696888\n",
      "Epoch 22, Loss: 0.02855145333854243\n",
      "Epoch 23, Loss: 0.028565347528132796\n",
      "Epoch 24, Loss: 0.028486514863552546\n",
      "After interaction 9, reward = 95.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n",
      "Epoch 0, Loss: 0.029646501437686046\n",
      "Epoch 1, Loss: 0.029590958476989275\n",
      "Epoch 2, Loss: 0.029482002682073793\n",
      "Epoch 3, Loss: 0.02944314889516388\n",
      "Epoch 4, Loss: 0.029499069253164523\n",
      "Epoch 5, Loss: 0.029360776756970616\n",
      "Epoch 6, Loss: 0.02921836993359469\n",
      "Epoch 7, Loss: 0.029229474715987712\n",
      "Epoch 8, Loss: 0.02929855737281455\n",
      "Epoch 9, Loss: 0.029049073247647432\n",
      "Epoch 10, Loss: 0.029206503855021833\n",
      "Epoch 11, Loss: 0.029079336206602937\n",
      "Epoch 12, Loss: 0.029098593803684753\n",
      "Epoch 13, Loss: 0.028929860553838338\n",
      "Epoch 14, Loss: 0.029042976761705867\n",
      "Epoch 15, Loss: 0.028954356369331084\n",
      "Epoch 16, Loss: 0.02892433915233138\n",
      "Epoch 17, Loss: 0.028868255419430854\n",
      "Epoch 18, Loss: 0.02880515794323424\n",
      "Epoch 19, Loss: 0.028764006519599757\n",
      "Epoch 20, Loss: 0.028610604579174328\n",
      "Epoch 21, Loss: 0.028637589756368372\n",
      "Epoch 22, Loss: 0.02869386582282587\n",
      "Epoch 23, Loss: 0.028565608218125215\n",
      "Epoch 24, Loss: 0.02864477181153461\n",
      "After interaction 10, reward = 215.0\n",
      "Training the learner\n",
      "Training for 25 epochs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdagger\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/DAgger.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_disable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SPRING2024/CS4756/CS4756_FinalProj_SpaceInvader/dagger.py:41\u001b[0m, in \u001b[0;36minteract\u001b[0;34m(env, learner, expert, observations, actions, checkpoint_path, seed, num_epochs, tqdm_disable)\u001b[0m\n\u001b[1;32m     39\u001b[0m actions\u001b[38;5;241m.\u001b[39mextend(expert_actions)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter interaction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_learner_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtqdm_disable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SPRING2024/CS4756/CS4756_FinalProj_SpaceInvader/bc.py:56\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(learner, observations, actions, checkpoint_path, num_epochs, tqdm_disable)\u001b[0m\n\u001b[1;32m     54\u001b[0m batch \u001b[38;5;241m=\u001b[39m data[batch_start : batch_start \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     55\u001b[0m obs_batch, actions_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 56\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m act \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(actions_batch), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dagger\n",
    "\n",
    "dagger.interact(env, learner, agent, observations=observations, actions=actions, checkpoint_path=\"models/DAgger.pth\", seed=seed, num_epochs=25, tqdm_disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing\n",
      "Video saved as dagger_learner.avi\n",
      "Reward: 190.0\n"
     ]
    }
   ],
   "source": [
    "learner.load_state_dict(torch.load(\"models/DAgger.pth\"), strict=True)\n",
    "total_learner_reward = 0\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "visualize(learner, env, \"dagger_learner\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
