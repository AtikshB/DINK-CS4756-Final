{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import AtariDataset\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reseed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "seed = 42\n",
    "reseed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = AtariDataset(\"atari_v1\")\n",
    "observations, actions = dataloader.compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed=25):\n",
    "    env = gym.make(env_id, obs_type='grayscale', render_mode=None)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env\n",
    "env = make_env(\"SpaceInvaders-v0\", seed=seed)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bc import SpaceInvLearner\n",
    "import bc\n",
    "\n",
    "learner = SpaceInvLearner(env)\n",
    "\n",
    "bc.train(learner=learner, observations=observations, checkpoint_path=\"models/bc_learner.pth\", actions=actions, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_learner_reward = 0\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_learner_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(total_learner_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD EXPERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expert.ppo import PPOAgent, ActorCnn, CriticCnn\n",
    "\n",
    "INPUT_SHAPE = (4, 84, 84)\n",
    "ACTION_SIZE = env.action_space.n\n",
    "SEED = 0\n",
    "GAMMA = 0.99           # discount factor\n",
    "ALPHA= 0.0001          # Actor learning rate\n",
    "BETA = 0.0001          # Critic learning rate\n",
    "TAU = 0.95\n",
    "BATCH_SIZE = 32\n",
    "PPO_EPOCH = 5\n",
    "CLIP_PARAM = 0.2\n",
    "UPDATE_EVERY = 1000     # how often to update the network \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = PPOAgent(INPUT_SHAPE, ACTION_SIZE, SEED, device, GAMMA, ALPHA, BETA, TAU, UPDATE_EVERY, BATCH_SIZE, PPO_EPOCH, CLIP_PARAM, ActorCnn(INPUT_SHAPE, ACTION_SIZE), CriticCnn(INPUT_SHAPE))\n",
    "agent.load_model(\"models/expert_actor.pth\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAgger Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dagger\n",
    "\n",
    "dagger.interact(env, learner, agent, observations=[], actions=[], checkpoint_path=\"models/DAgger.pth\", seed=seed, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import DQN\n",
    "import dqn\n",
    "\n",
    "INPUT_SHAPE = (210, 160)\n",
    "ACTION_SIZE = env.action_space.n\n",
    "\n",
    "dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(dqn_learner, env, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
