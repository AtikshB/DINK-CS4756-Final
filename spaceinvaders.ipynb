{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import AtariDataset\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reseed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "seed = 42\n",
    "reseed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[1960, 1870, 1770]\n"
     ]
    }
   ],
   "source": [
    "dataloader = AtariDataset(\"atari_v1\", 2)\n",
    "observations, actions, rewards, next_observations, dones = dataloader.compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(210, 160)\n",
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_id, seed=25):\n",
    "    env = gym.make(env_id, obs_type='grayscale', render_mode='rgb_array', repeat_action_probability=0.15,frameskip=1)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env\n",
    "env = make_env(\"SpaceInvaders-v0\", seed=seed)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(learner, env, video_name=\"test\"):\n",
    "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
    "\n",
    "        Args:\n",
    "            algorithm (PolicyGradient): Algorithm whose policy network will be rolled out for the episode. If\n",
    "            no algorithm is passed in, a random policy will be visualized.\n",
    "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
    "            when running on local machine.\n",
    "    \"\"\"\n",
    "\n",
    "    import cv2\n",
    "\n",
    "    print(\"Visualizing\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video = cv2.VideoWriter(f\"{video_name}.avi\", fourcc, 24, (160,210), isColor = True)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        im = env.render(mode='rgb_array')\n",
    "        \n",
    "        video.write(im)\n",
    "\n",
    "    video.release()\n",
    "    env.close()\n",
    "    print(f\"Video saved as {video_name}.avi\")\n",
    "    print(\"Reward: \" + str(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN DQN (TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from dqn import DQN\n",
    "# import dqn\n",
    "\n",
    "# INPUT_SHAPE = (210, 160)\n",
    "# ACTION_SIZE = env.action_space.n\n",
    "\n",
    "# dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "# dqn.train(dqn_learner, env, observations=observations, actions=actions, rewards=rewards, next_observations=next_observations, dones=dones, save_path='models/dqn_test.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the learner\n",
      "Training for 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:15<26:00, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.30219602271150975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:31<25:22, 15.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.15456958267999046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:46<24:54, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.14067100293862336\n"
     ]
    }
   ],
   "source": [
    "from bc import SpaceInvLearner\n",
    "import bc\n",
    "\n",
    "learner = SpaceInvLearner(env)\n",
    "\n",
    "bc.train(learner=learner, observations=observations, checkpoint_path=\"models/bc_learner.pth\", actions=actions, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonon\\AppData\\Local\\Temp\\ipykernel_21084\\1553395103.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  action = learner.get_action(torch.Tensor([obs]).to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as bc_learner.avi\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "learner.load_state_dict(torch.load(\"models/bc_learner.pth\"), strict=True)\n",
    "total_learner_reward = 0\n",
    "done = False\n",
    "\n",
    "# for i in range(20):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         with torch.no_grad():\n",
    "#             action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         total_learner_reward += reward\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "# print(total_learner_reward/20)\n",
    "\n",
    "visualize(learner, env, \"bc_learner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD EXPERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expert.ppo import PPOAgent, ActorCnn, CriticCnn\n",
    "\n",
    "INPUT_SHAPE = (4, 84, 84)\n",
    "ACTION_SIZE = env.action_space.n\n",
    "SEED = 0\n",
    "GAMMA = 0.99           # discount factor\n",
    "ALPHA= 0.00001         # Actor learning rate\n",
    "BETA = 0.00001          # Critic learning rate\n",
    "TAU = 0.95\n",
    "BATCH_SIZE = 64\n",
    "PPO_EPOCH = 10\n",
    "CLIP_PARAM = 0.2\n",
    "UPDATE_EVERY = 1000    # how often to update the network \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = PPOAgent(INPUT_SHAPE, ACTION_SIZE, SEED, device, GAMMA, ALPHA, BETA, TAU, UPDATE_EVERY, BATCH_SIZE, PPO_EPOCH, CLIP_PARAM, ActorCnn(INPUT_SHAPE, ACTION_SIZE), CriticCnn(INPUT_SHAPE))\n",
    "agent.load_model(\"models/expert_actor.pth\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAgger Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After interaction 0, reward = 0.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.35487094736701746\n",
      "Epoch 1, Loss: 0.2857381035274547\n",
      "Epoch 2, Loss: 0.2684738138952841\n",
      "Epoch 3, Loss: 0.2645116412467475\n",
      "Epoch 4, Loss: 0.26340705111138657\n",
      "Epoch 5, Loss: 0.26341036870161116\n",
      "Epoch 6, Loss: 0.2624255468914225\n",
      "Epoch 7, Loss: 0.25903930108900103\n",
      "Epoch 8, Loss: 0.25154174764233805\n",
      "Epoch 9, Loss: 0.2485266792322324\n",
      "Epoch 10, Loss: 0.24379143885972268\n",
      "Epoch 11, Loss: 0.23559911291952168\n",
      "Epoch 12, Loss: 0.22538839310730407\n",
      "Epoch 13, Loss: 0.21524493642662407\n",
      "Epoch 14, Loss: 0.19536946695634175\n",
      "Epoch 15, Loss: 0.16678819236772585\n",
      "Epoch 16, Loss: 0.15229528098760528\n",
      "Epoch 17, Loss: 0.1451131922566073\n",
      "Epoch 18, Loss: 0.14221671982147202\n",
      "Epoch 19, Loss: 0.14242760058129308\n",
      "Epoch 20, Loss: 0.14339944529834636\n",
      "Epoch 21, Loss: 0.13985652510439875\n",
      "Epoch 22, Loss: 0.13796875419814664\n",
      "Epoch 23, Loss: 0.13630981119315977\n",
      "Epoch 24, Loss: 0.1351237023027365\n",
      "Epoch 25, Loss: 0.1351900327722088\n",
      "Epoch 26, Loss: 0.1342277075947407\n",
      "Epoch 27, Loss: 0.1331115277558027\n",
      "Epoch 28, Loss: 0.13532313524816011\n",
      "Epoch 29, Loss: 0.13263614814634359\n",
      "Epoch 30, Loss: 0.13421577003243168\n",
      "Epoch 31, Loss: 0.1342128245623964\n",
      "Epoch 32, Loss: 0.13333750316597495\n",
      "Epoch 33, Loss: 0.13262214288384475\n",
      "Epoch 34, Loss: 0.13220153004468993\n",
      "Epoch 35, Loss: 0.13226936845788026\n",
      "Epoch 36, Loss: 0.13261628635093192\n",
      "Epoch 37, Loss: 0.1321055106953163\n",
      "Epoch 38, Loss: 0.1317799779912625\n",
      "Epoch 39, Loss: 0.1315895954086462\n",
      "After interaction 1, reward = 150.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.16434316067632618\n",
      "Epoch 1, Loss: 0.14681152540430562\n",
      "Epoch 2, Loss: 0.14205149613462334\n",
      "Epoch 3, Loss: 0.14058829786165208\n",
      "Epoch 4, Loss: 0.14018437528442884\n",
      "Epoch 5, Loss: 0.13829184158049174\n",
      "Epoch 6, Loss: 0.13754531661160813\n",
      "Epoch 7, Loss: 0.13725059899685307\n",
      "Epoch 8, Loss: 0.1359770076147886\n",
      "Epoch 9, Loss: 0.13652330320743764\n",
      "Epoch 10, Loss: 0.1363284193859132\n",
      "Epoch 11, Loss: 0.13561754076319427\n",
      "Epoch 12, Loss: 0.1345317056856305\n",
      "Epoch 13, Loss: 0.1344353194009362\n",
      "Epoch 14, Loss: 0.13494813608345563\n",
      "Epoch 15, Loss: 0.13369633141723572\n",
      "Epoch 16, Loss: 0.13219233953588566\n",
      "Epoch 17, Loss: 0.1319043731930628\n",
      "Epoch 18, Loss: 0.13273940644512286\n",
      "Epoch 19, Loss: 0.13147029889752115\n",
      "Epoch 20, Loss: 0.13214330207848726\n",
      "Epoch 21, Loss: 0.1310044495307346\n",
      "Epoch 22, Loss: 0.13111062045189883\n",
      "Epoch 23, Loss: 0.1307057861106441\n",
      "Epoch 24, Loss: 0.1306063962514296\n",
      "Epoch 25, Loss: 0.13232048253300366\n",
      "Epoch 26, Loss: 0.12889358222878344\n",
      "Epoch 27, Loss: 0.12961046729916684\n",
      "Epoch 28, Loss: 0.12968262509557454\n",
      "Epoch 29, Loss: 0.12797144759203363\n",
      "Epoch 30, Loss: 0.1267284816031019\n",
      "Epoch 31, Loss: 0.12746414209967502\n",
      "Epoch 32, Loss: 0.1276893372086825\n",
      "Epoch 33, Loss: 0.12699827314358325\n",
      "Epoch 34, Loss: 0.1266906690144716\n",
      "Epoch 35, Loss: 0.12647952435924237\n",
      "Epoch 36, Loss: 0.1274787772302289\n",
      "Epoch 37, Loss: 0.12705396814694786\n",
      "Epoch 38, Loss: 0.1254850286931956\n",
      "Epoch 39, Loss: 0.12479704096728765\n",
      "After interaction 2, reward = 110.0\n",
      "Training the learner\n",
      "Training for 40 epochs\n",
      "Epoch 0, Loss: 0.15029751414885836\n",
      "Epoch 1, Loss: 0.1375778072380081\n",
      "Epoch 2, Loss: 0.13531899512093026\n",
      "Epoch 3, Loss: 0.13471617768889055\n",
      "Epoch 4, Loss: 0.13389805050763157\n",
      "Epoch 5, Loss: 0.13248618145619678\n",
      "Epoch 6, Loss: 0.13291157176269502\n",
      "Epoch 7, Loss: 0.13178864128409815\n",
      "Epoch 8, Loss: 0.13229409129272438\n",
      "Epoch 9, Loss: 0.1306586133574048\n",
      "Epoch 10, Loss: 0.13037808908843285\n",
      "Epoch 11, Loss: 0.13161606692493438\n",
      "Epoch 12, Loss: 0.12867834288375685\n",
      "Epoch 13, Loss: 0.12833644395159388\n",
      "Epoch 14, Loss: 0.1282904978596472\n",
      "Epoch 15, Loss: 0.12995632398466483\n",
      "Epoch 16, Loss: 0.12803989460144757\n",
      "Epoch 17, Loss: 0.12780630588941655\n",
      "Epoch 18, Loss: 0.1270973277082136\n",
      "Epoch 19, Loss: 0.1265530152189446\n",
      "Epoch 20, Loss: 0.12638427137081662\n",
      "Epoch 21, Loss: 0.12744909107783528\n",
      "Epoch 22, Loss: 0.1261951039554723\n",
      "Epoch 23, Loss: 0.1251151471358011\n",
      "Epoch 24, Loss: 0.1249304367529273\n",
      "Epoch 25, Loss: 0.12476714340748946\n",
      "Epoch 26, Loss: 0.12522619875738059\n",
      "Epoch 27, Loss: 0.12467365284334225\n",
      "Epoch 28, Loss: 0.1243202801319052\n",
      "Epoch 29, Loss: 0.12397065727122716\n",
      "Epoch 30, Loss: 0.12261688453523442\n",
      "Epoch 31, Loss: 0.12311243390761359\n",
      "Epoch 32, Loss: 0.12293168002503194\n"
     ]
    }
   ],
   "source": [
    "import dagger\n",
    "\n",
    "dagger.interact(env, learner, agent, observations=[], actions=[], checkpoint_path=\"models/DAgger.pth\", seed=seed, num_epochs=40, tqdm_disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118.25\n"
     ]
    }
   ],
   "source": [
    "learner.load_state_dict(torch.load(\"models/DAgger.pth\"), strict=True)\n",
    "total_learner_reward = 0\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(20):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_learner_reward += reward\n",
    "\n",
    "print(total_learner_reward/20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
