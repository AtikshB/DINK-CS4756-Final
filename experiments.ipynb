{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import AtariDataset\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from dqn import DQN\n",
    "import dqn\n",
    "\n",
    "def reseed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "seed = 42\n",
    "reseed(seed)\n",
    "\n",
    "def make_env(env_id, seed=25):\n",
    "    env = gym.make(env_id, obs_type='grayscale', render_mode=None, repeat_action_probability=0.0,frameskip=1)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env\n",
    "env = make_env(\"SpaceInvaders-v0\", seed=seed)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD ATARI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = AtariDataset(\"atari_v1\", 15)\n",
    "atari_obs, atari_act, atari_rew, atari_next, atari_done = dataloader.compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collect(learner, env, num_episodes, save_path, device):\n",
    "  observations = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  next_observations = []\n",
    "  dones = []\n",
    "  for _ in range(num_episodes):\n",
    "      obs = env.reset()\n",
    "      done = False\n",
    "      while not done:\n",
    "          if isinstance(learner, DQN):\n",
    "            with torch.no_grad():\n",
    "              action = learner.get_action(\n",
    "              torch.tensor(obs).to(device).unsqueeze(0), eps=0.0\n",
    "              )\n",
    "          else:\n",
    "             with torch.no_grad():\n",
    "              action = learner.get_action(\n",
    "              torch.tensor(obs).to(device).unsqueeze(0)\n",
    "              )\n",
    "          next_obs, reward, done, _ = env.step(action)\n",
    "          observations.append(obs.flatten())\n",
    "          actions.append(action)\n",
    "          rewards.append(reward)\n",
    "          next_observations.append(next_obs.flatten())\n",
    "          dones.append(done)\n",
    "          obs = next_obs\n",
    "  return observations, actions, rewards, next_observations, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bc import SpaceInvLearner\n",
    "\n",
    "bc_learner = SpaceInvLearner(env)\n",
    "\n",
    "bc_learner.load_state_dict(torch.load('models/bc_learner.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_obs, bc_act, bc_rew, bc_next, bc_done = data_collect(bc_learner, env, 25, 'numpy_data/bc', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_learner = SpaceInvLearner(env)\n",
    "\n",
    "dagger_learner.load_state_dict(torch.load('models/DAgger.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_obs, dagger_act, dagger_rew, dagger_next, dagger_done = data_collect(dagger_learner, env, 25, 'numpy_data/dagger', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(values, label):\n",
    "    plt.plot(np.arange(len(values)), values)\n",
    "    plt.ylabel(label)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN DQN w/ ATARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import DQN\n",
    "import dqn\n",
    "\n",
    "INPUT_SHAPE = 210*160\n",
    "ACTION_SIZE = env.action_space.n\n",
    "\n",
    "atari_dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(atari_dqn_learner, env, observations=atari_obs, actions=atari_act, rewards=atari_rew, next_observations=atari_next, dones=atari_done, save_path='models/atari_dqn.pth', num_episodes=25, lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(atari_dqn_learner.test_loss, 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(atari_dqn_learner.test_scores, 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN DQN w/ BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(bc_dqn_learner, env, observations=bc_obs, actions=bc_act, rewards=bc_rew, next_observations=bc_next, dones=bc_done, save_path='models/bc_dqn.pth', num_episodes=25, lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(bc_dqn_learner.test_loss, 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(bc_dqn_learner.test_scores, 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN DQN w/ DAgger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(dagger_dqn_learner, env, observations=dagger_obs, actions=dagger_act, rewards=dagger_rew, next_observations=dagger_next, dones=dagger_done, save_path='models/dagger_dqn.pth', num_episodes=25, lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(dagger_dqn_learner.test_loss, 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(dagger_dqn_learner.test_scores, 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn_learner.load_state_dict(torch.load('models/atari_dqn.pth'), strict=True)\n",
    "\n",
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = dqn_learner.get_action(torch.Tensor([obs]).to(device), eps=0.0)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "atari_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('AGC DQN', atari_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_learner.load_state_dict(torch.load('models/bc_dqn.pth'), strict=True)\n",
    "\n",
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = dqn_learner.get_action(torch.Tensor([obs]).to(device), eps=0.0)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "bc_dqn_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('BC DQN', bc_dqn_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_learner.load_state_dict(torch.load('models/dagger_dqn.pth'), strict=True)\n",
    "\n",
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = dqn_learner.get_action(torch.Tensor([obs]).to(device), eps=0.0)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "dagger_dqn_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('DAgger DQN', dagger_dqn_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = bc_learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "bc_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('BC', bc_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = dagger_learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "bc_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('DAgger', bc_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [item[0] for item in avg_scores]\n",
    "scores = [item[1] for item in avg_scores]\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.bar(names, scores, color='skyblue')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Average scores over 50 runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
