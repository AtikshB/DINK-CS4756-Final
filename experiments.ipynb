{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import AtariDataset\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "def reseed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "seed = 42\n",
    "reseed(seed)\n",
    "\n",
    "def make_env(env_id, seed=25):\n",
    "    env = gym.make(env_id, obs_type='grayscale', render_mode=None, repeat_action_probability=0.15,frameskip=1)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env\n",
    "env = make_env(\"SpaceInvaders-v0\", seed=seed)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(learner, env, video_name=\"test\"):\n",
    "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
    "\n",
    "        Args:\n",
    "            algorithm (PolicyGradient): Algorithm whose policy network will be rolled out for the episode. If\n",
    "            no algorithm is passed in, a random policy will be visualized.\n",
    "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
    "            when running on local machine.\n",
    "    \"\"\"\n",
    "\n",
    "    import cv2\n",
    "\n",
    "    print(\"Visualizing\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video = cv2.VideoWriter(f\"{video_name}.avi\", fourcc, 24, (160,210), isColor = True)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        im = env.render(mode='rgb_array')\n",
    "        \n",
    "        video.write(im)\n",
    "\n",
    "    video.release()\n",
    "    env.close()\n",
    "    print(f\"Video saved as {video_name}.avi\")\n",
    "    print(\"Reward: \" + str(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD ATARI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = AtariDataset(\"atari_v1\")\n",
    "observations, actions, rewards, next_observations, dones = dataloader.compile_data()\n",
    "\n",
    "np.save('numpy_data/atari/observations', observations)\n",
    "np.save('numpy_data/atari/actions', actions)\n",
    "np.save('numpy_data/atari/rewards', rewards)\n",
    "np.save('numpy_data/atari/dones', dones)\n",
    "np.save('numpy_data/atari/next_observations', next_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari_obs = np.load('numpy_data/atari/observations')\n",
    "atari_act = np.load('numpy_data/atari/actions')\n",
    "atari_rew = np.load('numpy_data/atari/rewards')\n",
    "atari_next = np.load('numpy_data/atari/dones')\n",
    "atari_done = np.load('numpy_data/atari/next_observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collect(learner, env, num_episodes, save_path):\n",
    "  observations = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  next_observations = []\n",
    "  dones = []\n",
    "  for _ in range(num_episodes):\n",
    "      obs = env.reset()\n",
    "      done = False\n",
    "      while not done:\n",
    "          action = learner.get_action(\n",
    "              torch.tensor(obs).unsqueeze(0), eps=0.0\n",
    "          )  # Greedy action\n",
    "          next_obs, reward, done, _ = env.step(action)\n",
    "          observations.append(obs)\n",
    "          actions.append(action)\n",
    "          rewards.append(reward)\n",
    "          next_observations.append(next_obs)\n",
    "          dones.append(done)\n",
    "          obs = next_obs\n",
    "  np.save(os.path.join(save_path, 'observations'), observations)\n",
    "  np.save(os.path.join(save_path, 'actions'), actions)\n",
    "  np.save(os.path.join(save_path, 'rewards'), rewards)\n",
    "  np.save(os.path.join(save_path, 'dones'), dones)\n",
    "  np.save(os.path.join(save_path, 'next_observations'), next_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bc import SpaceInvLearner\n",
    "\n",
    "bc_learner = SpaceInvLearner(env)\n",
    "\n",
    "bc_learner.load_state_dict(torch.load('models/bc_learner.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collect(bc_learner, env, 25, 'numpy_data/bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_learner = SpaceInvLearner(env)\n",
    "\n",
    "dagger_learner.load_state_dict(torch.load('models/DAgger.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collect(bc_learner, env, 25, 'numpy_data/dagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN DQN w/ ATARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn import DQN\n",
    "import dqn\n",
    "\n",
    "INPUT_SHAPE = 210*160\n",
    "ACTION_SIZE = env.action_space.n\n",
    "\n",
    "atari_dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(atari_dqn_learner, env, observations=atari_obs, actions=atari_act, rewards=atari_rew, next_observations=atari_next, dones=atari_done, save_path='models/atari_dqn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN DQN w/ BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_obs = np.load('numpy_data/bc/observations')\n",
    "bc_act = np.load('numpy_data/bc/actions')\n",
    "bc_rew = np.load('numpy_data/bc/rewards')\n",
    "bc_done = np.load('numpy_data/bc/dones')\n",
    "bc_next = np.load('numpy_data/bc/next_observations')\n",
    "\n",
    "bc_dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(bc_dqn_learner, env, observations=bc_obs, actions=bc_act, rewards=bc_rew, next_observations=bc_next, dones=bc_done, save_path='models/bc_dqn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN DQN w/ DAgger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_obs = np.load('numpy_data/dagger/observations')\n",
    "dagger_act = np.load('numpy_data/dagger/actions')\n",
    "dagger_rew = np.load('numpy_data/dagger/rewards')\n",
    "dagger_done = np.load('numpy_data/dagger/dones')\n",
    "dagger_next = np.load('numpy_data/dagger/next_observations')\n",
    "\n",
    "dagger_dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(dagger_dqn_learner, env, observations=dagger_obs, actions=dagger_act, rewards=dagger_rew, next_observations=dagger_done, dones=dagger_next, save_path='models/dagger_dqn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn_learner.load_state_dict(torch.load('models/atari_dqn.pth'), strict=True)\n",
    "\n",
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(20):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = dqn_learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "print(np.mean(total_learner_reward)/20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
