{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(210, 160)\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from dataloader import AtariDataset\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from dqn import DQN\n",
    "import dqn\n",
    "\n",
    "def reseed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "seed = 42\n",
    "reseed(seed)\n",
    "\n",
    "def make_env(env_id, seed=25):\n",
    "    env = gym.make(env_id, obs_type='grayscale', render_mode=None, repeat_action_probability=0.15,frameskip=1)\n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "    return env\n",
    "env = make_env(\"SpaceInvaders-v0\", seed=seed)\n",
    "print(env.action_space.n)\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(learner, env, video_name=\"test\"):\n",
    "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
    "\n",
    "        Args:\n",
    "            algorithm (PolicyGradient): Algorithm whose policy network will be rolled out for the episode. If\n",
    "            no algorithm is passed in, a random policy will be visualized.\n",
    "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
    "            when running on local machine.\n",
    "    \"\"\"\n",
    "\n",
    "    import cv2\n",
    "\n",
    "    print(\"Visualizing\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    video = cv2.VideoWriter(f\"{video_name}.avi\", fourcc, 24, (160,210), isColor = True)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        im = env.render(mode='rgb_array')\n",
    "        \n",
    "        video.write(im)\n",
    "\n",
    "    video.release()\n",
    "    env.close()\n",
    "    print(f\"Video saved as {video_name}.avi\")\n",
    "    print(\"Reward: \" + str(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD ATARI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[1960, 1870, 1770, 1705, 1700, 1685, 1665, 1660, 1660, 1605, 1605, 1580, 1525, 1490, 1470]\n"
     ]
    }
   ],
   "source": [
    "dataloader = AtariDataset(\"atari_v1\", 15)\n",
    "observations, actions, rewards, next_observations, dones = dataloader.compile_data()\n",
    "\n",
    "np.save('numpy_data/atari/observations', observations)\n",
    "np.save('numpy_data/atari/actions', actions)\n",
    "np.save('numpy_data/atari/rewards', rewards)\n",
    "np.save('numpy_data/atari/dones', dones)\n",
    "np.save('numpy_data/atari/next_observations', next_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari_obs = np.load('numpy_data/atari/observations.npy')\n",
    "atari_act = np.load('numpy_data/atari/actions.npy')\n",
    "atari_rew = np.load('numpy_data/atari/rewards.npy')\n",
    "atari_next = np.load('numpy_data/atari/dones.npy')\n",
    "atari_done = np.load('numpy_data/atari/next_observations.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collect(learner, env, num_episodes, save_path, device):\n",
    "  observations = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  next_observations = []\n",
    "  dones = []\n",
    "  for _ in range(num_episodes):\n",
    "      obs = env.reset()\n",
    "      done = False\n",
    "      while not done:\n",
    "          if isinstance(learner, DQN):\n",
    "            with torch.no_grad():\n",
    "              action = learner.get_action(\n",
    "              torch.tensor(obs).to(device).unsqueeze(0), eps=0.0\n",
    "              )\n",
    "          else:\n",
    "             with torch.no_grad():\n",
    "              action = learner.get_action(\n",
    "              torch.tensor(obs).to(device).unsqueeze(0)\n",
    "              )\n",
    "          next_obs, reward, done, _ = env.step(action)\n",
    "          observations.append(obs)\n",
    "          actions.append(action)\n",
    "          rewards.append(reward)\n",
    "          next_observations.append(next_obs)\n",
    "          dones.append(done)\n",
    "          obs = next_obs\n",
    "  np.save(os.path.join(save_path, 'observations'), observations)\n",
    "  np.save(os.path.join(save_path, 'actions'), actions)\n",
    "  np.save(os.path.join(save_path, 'rewards'), rewards)\n",
    "  np.save(os.path.join(save_path, 'dones'), dones)\n",
    "  np.save(os.path.join(save_path, 'next_observations'), next_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bc import SpaceInvLearner\n",
    "\n",
    "bc_learner = SpaceInvLearner(env)\n",
    "\n",
    "bc_learner.load_state_dict(torch.load('models/bc_learner.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collect(bc_learner, env, 25, 'numpy_data/bc', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dagger_learner = SpaceInvLearner(env)\n",
    "\n",
    "dagger_learner.load_state_dict(torch.load('models/DAgger.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collect(dagger_learner, env, 25, 'numpy_data/dagger', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(values, label):\n",
    "    plt.plot(np.arange(len(values)), values)\n",
    "    plt.ylabel(label)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN DQN w/ ATARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (128,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m ACTION_SIZE \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m      7\u001b[0m atari_dqn_learner \u001b[38;5;241m=\u001b[39m DQN(INPUT_SHAPE, ACTION_SIZE)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43matari_dqn_learner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matari_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matari_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matari_rew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_observations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matari_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matari_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/atari_dqn.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonon\\Documents\\Robot Learning\\CS4756_FinalProj_SpaceInvader\\dqn.py:148\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(network, env, observations, actions, rewards, next_observations, dones, save_path, batch_size, num_episodes, lr, add_data_every, device)\u001b[0m\n\u001b[0;32m    141\u001b[0m actions_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m    142\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(actions_batch), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m    143\u001b[0m )\n\u001b[0;32m    144\u001b[0m rewards_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m    145\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(rewards_batch), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m    146\u001b[0m )\n\u001b[0;32m    147\u001b[0m next_states_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m--> 148\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states_batch\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    150\u001b[0m dones_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m    151\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(dones_batch), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m    152\u001b[0m )\n\u001b[0;32m    154\u001b[0m q_vals \u001b[38;5;241m=\u001b[39m network(obs_batch)[\n\u001b[0;32m    155\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(actions_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), actions_batch\n\u001b[0;32m    156\u001b[0m ]\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (128,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from dqn import DQN\n",
    "import dqn\n",
    "\n",
    "INPUT_SHAPE = 210*160\n",
    "ACTION_SIZE = env.action_space.n\n",
    "\n",
    "atari_dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(atari_dqn_learner, env, observations=atari_obs, actions=atari_act, rewards=atari_rew, next_observations=atari_next, dones=atari_done, save_path='models/atari_dqn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(atari_dqn_learner.test_loss, 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(atari_dqn_learner.test_scores, 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN DQN w/ BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_obs = np.load('numpy_data/bc/observations.npy')\n",
    "bc_act = np.load('numpy_data/bc/actions.npy')\n",
    "bc_rew = np.load('numpy_data/bc/rewards.npy')\n",
    "bc_done = np.load('numpy_data/bc/dones.npy')\n",
    "bc_next = np.load('numpy_data/bc/next_observations.npy')\n",
    "\n",
    "bc_dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(bc_dqn_learner, env, observations=bc_obs, actions=bc_act, rewards=bc_rew, next_observations=bc_next, dones=bc_done, save_path='models/bc_dqn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(bc_dqn_learner.test_loss, 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(bc_dqn_learner.test_scores, 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN DQN w/ DAgger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_obs = np.load('numpy_data/dagger/observations.npy')\n",
    "dagger_act = np.load('numpy_data/dagger/actions.npy')\n",
    "dagger_rew = np.load('numpy_data/dagger/rewards.npy')\n",
    "dagger_done = np.load('numpy_data/dagger/dones.npy')\n",
    "dagger_next = np.load('numpy_data/dagger/next_observations.npy')\n",
    "\n",
    "dagger_dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn.train(dagger_dqn_learner, env, observations=dagger_obs, actions=dagger_act, rewards=dagger_rew, next_observations=dagger_done, dones=dagger_next, save_path='models/dagger_dqn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(dagger_dqn_learner.test_loss, 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(dagger_dqn_learner.test_scores, 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_learner = DQN(INPUT_SHAPE, ACTION_SIZE)\n",
    "\n",
    "dqn_learner.load_state_dict(torch.load('models/atari_dqn.pth'), strict=True)\n",
    "\n",
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = dqn_learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "atari_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('AGC DQN', atari_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_learner.load_state_dict(torch.load('models/bc_dqn.pth'), strict=True)\n",
    "\n",
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = dqn_learner.get_action(torch.Tensor([obs]).to(device), eps=0.0)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "bc_dqn_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('BC DQN', bc_dqn_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_learner.load_state_dict(torch.load('models/dagger_dqn.pth'), strict=True)\n",
    "\n",
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = dqn_learner.get_action(torch.Tensor([obs]).to(device), eps=0.0)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "dagger_dqn_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('DAgger DQN', dagger_dqn_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = bc_learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "bc_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('BC', bc_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_learner_reward = []\n",
    "done = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(50):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    sum_reward = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = dagger_learner.get_action(torch.Tensor([obs]).to(device))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        sum_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_learner_reward += [sum_reward]\n",
    "\n",
    "bc_mean = np.mean(total_learner_reward)\n",
    "avg_scores.append(('DAgger', bc_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [item[0] for item in avg_scores]\n",
    "scores = [item[1] for item in avg_scores]\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.bar(names, scores, color='skyblue')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Average scores over 50 runs')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
